{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output bucket to write to, dataproc service account must have write access\n",
    "# Do not include trailing slash or \"gs://\"\n",
    "output_bucket = \"clingen-dataproc-workspace-kferrite\"\n",
    "# Set the TSV path to write into bucket. Can contain slash like \"folder/file.tsv\"\n",
    "# Do not include leading slash\n",
    "report_filename = \"clinvar-annotation-af-revstatus.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "# `idempontent=True` is useful for running all cells in the notebook\n",
    "hl.init(idempotent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions for file placement\n",
    "import subprocess\n",
    "import os\n",
    "import time, datetime\n",
    "\n",
    "def run_args(args, fail_on_stderr=False, success_codes=[0]) -> tuple: # (stdout,stderr,returncode)\n",
    "    print(args)\n",
    "    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "    if (fail_on_stderr and len(stderr) > 0) or (p.returncode not in success_codes):\n",
    "        raise RuntimeError(\"command {} failed with code {}:{}\".format(\n",
    "            args, p.returncode, stderr))\n",
    "    return (stdout, stderr, p.returncode)\n",
    "\n",
    "def local_to_bucket(local_path:str, gcs_path:str):\n",
    "    if not gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = \"gs://{}/{}\".format(output_bucket, gcs_path)\n",
    "    args = [\"gsutil\", \"cp\", local_path, gcs_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def bucket_to_local(gcs_path:str, local_path:str):\n",
    "    if not gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = \"gs://{}/{}\".format(output_bucket, gcs_path)\n",
    "    args = [\"gsutil\", \"cp\", gcs_path, local_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def local_to_hdfs(local_path:str, hdfs_path:str):\n",
    "    if not local_path.startswith(\"/\"):\n",
    "        local_path = os.path.join(os.getcwd(), local_path)\n",
    "    args = [\"hdfs\", \"dfs\", \"-rm\", hdfs_path]\n",
    "    run_args(args, success_codes=[0,1]) # Allow error\n",
    "    args = [\"hadoop\", \"fs\", \"-cp\", \"file://\" + local_path, hdfs_path]\n",
    "#     args = [\"hdfs\", \"dfs\", \"-cp\", \"file://\" + local_path, hdfs_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def hdfs_to_local(hdfs_path:str, local_path:str):\n",
    "    if os.path.exists(local_path):\n",
    "        os.remove(local_path)\n",
    "    args = [\"hdfs\", \"dfs\", \"-cp\", hdfs_path, \"file://\" + local_path]\n",
    "    run_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain desired thresholds\n",
    "import io, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_filename = \"input_files/sheet3.tsv\"\n",
    "bucket_to_local(input_filename, input_filename)\n",
    "# print(os.path.isfile(input_filename))\n",
    "# local_to_hdfs(input_filename, input_filename)\n",
    "\n",
    "pd_df = pd.read_csv(input_filename, sep='\\t', dtype=str)\n",
    "pd_df = pd_df.astype(object).replace(np.nan, None)\n",
    "input_ds = hl.Table.from_pandas(pd_df)\n",
    "input_ds = input_ds.persist()\n",
    "input_ds.describe()\n",
    "input_ds.show()\n",
    "# with open(input_filename) as f_in:\n",
    "#     variation_ids = [line.strip() for line in f_in if len(line) > 0]\n",
    "print(\"Loaded {} variations\".format(input_ds.count()))\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "# Read gnomAD data as Hail Tables\n",
    "# ds_exomes = hl.read_table(\n",
    "#     \"gs://gcp-public-data--gnomad/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\")\n",
    "# ds_exomes = ds_exomes.annotate(\n",
    "#     source=\"gnomAD Exomes\")\n",
    "\n",
    "# ds_genomes = hl.read_table(\n",
    "#     \"gs://gcp-public-data--gnomad/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht\")\n",
    "ds_genomes = hl.read_table(\n",
    "    \"gs://gcp-public-data--gnomad/release/3.1/ht/genomes/gnomad.genomes.v3.1.sites.ht\")\n",
    "# ds_genomes = ds_genomes.annotate(\n",
    "#     source=\"gnomAD Genomes\")\n",
    "\n",
    "# Can perform a union here if wanting both (ds = ds1.union(ds2))\n",
    "def select_necessary_cols(ds):\n",
    "    ds = ds.select(ds.freq, ds.faf, ds.vep)\n",
    "    return ds\n",
    "\n",
    "# gnomad_exomes = select_necessary_cols(ds_exomes)\n",
    "gnomad_genomes = select_necessary_cols(ds_genomes)\n",
    "\n",
    "# gnomad = gnomad_genomes.union(gnomad_exomes, unify=True)\n",
    "gnomad = gnomad_genomes\n",
    "\n",
    "# Persist to disk the filtered table and reload\n",
    "# gnomad = gnomad.checkpoint(\"hail-checkpoints/gnomad.genomes.v3.1.sites.ht\")\n",
    "# gnomad = gnomad.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnomad.describe()\n",
    "# gnomad.show()\n",
    "# If exists in genomes and exomes, pick genome\n",
    "# gnomad_locus_counts = (gnomad.group_by(gnomad.locus)\n",
    "#                             .aggregate(count=hl.agg.count()))\n",
    "# gnomad_locus_counts.show()\n",
    "\n",
    "# gnomad = gnomad.annotate(\n",
    "#     locus_count=gnomad.filter(gnomad.locus==gnomad.locus).count()\n",
    "# )\n",
    "\n",
    "\n",
    "# gnomad = gnomad.filter(\n",
    "#     (gnomad.source == hl.str(\"gnomAD Exomes\")) & (gnomad.filter(gnomad.locus==gnomad.locus).count() > 1)\n",
    "#     #hl.any(lambda genome: genome.locus == gnomad.locus, gnomad_genomes)\n",
    "\n",
    "# )\n",
    "\n",
    "# gnomad.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each correponds to a filter func for a (k,v) of faf label to value\n",
    "GNOMAD_SUPERPOP = lambda t: re.match(\"^[a-zA-Z]+-adj$\", t[0])\n",
    "ANY = lambda t: True\n",
    "\n",
    "# By default, filter to superpopulations aggregate faf\n",
    "def faf_filter(faf_idx_tuple:tuple):\n",
    "    return GNOMAD_SUPERPOP(faf_idx_tuple)\n",
    "\n",
    "print(hl.eval(gnomad.globals.faf_index_dict))\n",
    "\n",
    "list(filter(faf_filter, [(k,v) for k,v in hl.eval(gnomad.globals.faf_index_dict).items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ds.freq has raw frequency information, including AN, AC, and pop label. This is an array of \n",
    "structs, at indices determined by the categories in ds.globals.freq_index_dict\n",
    "\n",
    "ds.faf has filtered allele frequency information, including confidence intervals faf95 adn faf99.\n",
    "This is an array of structs, at indices determined by the category map in ds.globals.faf_index_dict\n",
    "\"\"\"\n",
    "\n",
    "def add_popmax_af(ds):\n",
    "    \"\"\"\n",
    "    Adds a popmax_faf and popmax_af_pop column to the ds Hail Table containing gnomAD fields.\n",
    "    \n",
    "    popmax_faf is a faf structure from the original ds, containing the maximum faf of the\n",
    "    listed faf structures in the original ds, based on the filtering criteria \n",
    "    `default_faf_filter_type`. \n",
    "    \n",
    "    The popmax_index_dict_key column contains the text field from the\n",
    "    ds.globals.faf_index_dict which corresponds to each popmax_faf. This is similar to the\n",
    "    ds.popmax_faf.meta[\"pop\"] value but not exactly the same (gnomad_afr vs afr)\n",
    "    \n",
    "    Returns the updated ds.\n",
    "    \"\"\"\n",
    "    # Identify indices in FAF field that correspond to the entire dataset (not a subset like non-cancer)\n",
    "    # faf_index_map = [(k,v) for k, v in hl.eval(ds.globals.faf_index_dict).items() if k.startswith(\"gnomad_\")]\n",
    "    # Each correponds to a filter func for a (k,v) of faf label to value\n",
    "    GNOMAD_v3_1_SUPERPOP = lambda t: re.match(\"^[a-zA-Z]+-adj$\", t[0])\n",
    "    ANY = lambda t: True\n",
    "\n",
    "    # By default, filter to superpopulations aggregate faf\n",
    "    def faf_filter(faf_idx_tuple:tuple):\n",
    "        return GNOMAD_v3_1_SUPERPOP(faf_idx_tuple)\n",
    "\n",
    "    print(hl.eval(gnomad.globals.faf_index_dict))\n",
    "\n",
    "    # Get list of the global faf_index_dict which meets the default_faf_filter criteria\n",
    "    # This gives the indices of the desired populations, by default will take all top level populations\n",
    "    faf_index_map = list(filter(faf_filter, [(k,v) for k,v in hl.eval(ds.globals.faf_index_dict).items()]))\n",
    "    faf_indices = [v for k,v in faf_index_map]\n",
    "    faf_labels = [k for k,v in faf_index_map]\n",
    "    \n",
    "    # Annotate table with popmax FAF\n",
    "    \n",
    "    # This only will return the maximum pop FAF for each\n",
    "    # variant, even if multiple populations meet the criteria. \n",
    "    # If we want all matching populations, need an explode() call\n",
    "    # to flatten the pop FAFs into a record per pop per variant\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "        # AF popmax meta.pop, faf95, faf99\n",
    "        popmax_faf=hl.sorted(\n",
    "            # Take only the FAF entries that correspond to the desired populations (faf_indices)\n",
    "            hl.literal(faf_indices).map(lambda i: ds.faf[i]),\n",
    "            # Sort by 95% confidence FAF\n",
    "            lambda faf_entry: faf_entry.faf95,\n",
    "            # Sort high to low\n",
    "            reverse=True\n",
    "        )[0] # Take the first entry with the highest FAF\n",
    "        ,\n",
    "        # Label of the freq_index_dict entry for this record's max pop\n",
    "        popmax_index_dict_key=hl.sorted(\n",
    "            # List of tuples of (poplabel, faf_index)\n",
    "            list(zip(list(faf_labels), list(faf_indices))),\n",
    "\n",
    "            # Take only the FAF entries that correspond to the entire dataset\n",
    "            # Sort by 95% confidence FAF\n",
    "            key=lambda tpl: ds.faf[tpl[1]].faf95,\n",
    "            # Sort high to low\n",
    "            reverse=True\n",
    "        )[0][0] # Take the first entry, which has the highest FAF\n",
    "    )\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "#         popmax_faf_pop_freq=ds.freq[ds.globals.freq_index_dict[\"gnomad_\" + ds.popmax_faf.meta.get(\"pop\")]]\n",
    "\n",
    "        # ds.globals.freq_index_dict uses the same keys as ds.globals.faf_index_dict so\n",
    "        # we can reuse ds.popmax_index_dict_key created above\n",
    "        # Frequency info about the AF popmax population (AC, AF, AN, homozygote_count)\n",
    "        popmax_faf_pop_freq=ds.freq[ds.globals.freq_index_dict[ds.popmax_index_dict_key]] \n",
    "    )\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "# gnomad_with_popmax = add_popmax_af(gnomad)\n",
    "# gnomad_with_popmax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ClinVar VCF as Hail Table\n",
    "# clinvar = hl.import_vcf(\"/path/to/clinvar.vcf.gz\", force_bgz=True, drop_samples=True, skip_invalid_loci=True).rows()\n",
    "\n",
    "# Download clinvar BGZF\n",
    "import os, requests, subprocess\n",
    "import gzip\n",
    "\n",
    "# Function to download a file to a localpath. ClinVar VCF is small enough to download to dataproc default local disk.\n",
    "def download_to_file(url, filepath):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(\"Failed to obtain ClinVar VCF:{}\\n{}\".format(r.status_code))\n",
    "    with open(filepath, \"wb\") as fout: \n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "             if chunk:\n",
    "                 fout.write(chunk)\n",
    "                \n",
    "def add_chr_to_vcf_contig(filepath:str) -> str:\n",
    "    \"\"\"\n",
    "    Takes vcf.gz file path, replaces contigs with chr+contig,\n",
    "    returns new vcf.gz file path.\n",
    "    \"\"\"\n",
    "    output_filepath = filepath.replace(\".gz\", \".1.gz\")\n",
    "    with gzip.open(filepath, \"rb\") as f_in:\n",
    "        with gzip.open(output_filepath, \"wb\") as f_out:\n",
    "            for line in f_in:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                terms = line.split(\"\\t\")\n",
    "                if terms[0] in ([str(i) for i in range(1,23)] + ['X','Y']):\n",
    "                    terms[0] = \"chr\" + terms[0]\n",
    "                line = \"\\t\".join(terms)\n",
    "                if not line.endswith(\"\\n\"):\n",
    "                    line = line + \"\\n\"\n",
    "                f_out.write(line.encode(\"utf-8\"))\n",
    "    return output_filepath\n",
    "                \n",
    "                \n",
    "# This url always points to the latest dump file, updated periodically by ClinVar\n",
    "clinvar_vcf_url = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz\"\n",
    "clinvar_vcf_localpath = \"/home/hail/clinvar.vcf.gz\"\n",
    "clinvar_vcf_hdfs = \"clinvar.vcf\"\n",
    "download_to_file(clinvar_vcf_url, clinvar_vcf_localpath)\n",
    "assert(os.path.exists(clinvar_vcf_localpath))\n",
    "print(\"Downloaded ClinVar VCF, file size (expecting ~30M): %d\" % os.path.getsize(clinvar_vcf_localpath))\n",
    "\n",
    "print(\"Updating contigs\")\n",
    "clinvar_fixed_localpath = add_chr_to_vcf_contig(clinvar_vcf_localpath)\n",
    "# Unzip it because no bgzip command here to put in blocked gzip\n",
    "def unzip(filepath:str) -> str:\n",
    "    assert(filepath.endswith(\".gz\"))\n",
    "    output_filepath = filepath[0:-3]\n",
    "    with gzip.open(filepath, \"rb\") as f_in:\n",
    "        with open(output_filepath, \"wb\") as f_out:\n",
    "            for line in f_in:\n",
    "                f_out.write(line)\n",
    "    return output_filepath\n",
    "clinvar_unzipped_fixed_localpath = unzip(clinvar_fixed_localpath)\n",
    "\n",
    "\n",
    "\n",
    "# Hail needs the file in HDFS\n",
    "local_to_hdfs(clinvar_unzipped_fixed_localpath, clinvar_vcf_hdfs)\n",
    "# p = subprocess.Popen([\"hdfs\", \"dfs\", \"-cp\", \"file://\" + clinvar_vcf_localpath, clinvar_vcf_hdfs])\n",
    "# print(p.communicate())\n",
    "\n",
    "clinvar = hl.import_vcf(\n",
    "    clinvar_vcf_hdfs,\n",
    "#     force_bgz=True,\n",
    "    drop_samples=True, \n",
    "    skip_invalid_loci=True,\n",
    "    reference_genome=\"GRCh38\"\n",
    ").rows()\n",
    "print(\"Imported {} records from ClinVar\".format(clinvar.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter clinvar to those in input set\n",
    "input_variation_ids = input_ds.clinvar_variation_id.collect()\n",
    "clinvar = clinvar.filter(\n",
    "    # Return true if clinvar.rsid value matches any input variation id \n",
    "    hl.any(lambda rec: rec == clinvar.rsid, input_variation_ids)\n",
    ")\n",
    "print(\"ClinVar filtered to {} rows\".format(clinvar.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar.describe()\n",
    "clinvar = clinvar.select(\n",
    "    clinvar.rsid,\n",
    "    info=hl.struct(\n",
    "        CLNSIG=clinvar.info.CLNSIG,\n",
    "        CLNSIGCONF=clinvar.info.CLNSIGCONF,\n",
    "        CLNREVSTAT=clinvar.info.CLNREVSTAT,\n",
    "        CLNDN=clinvar.info.CLNDN\n",
    "    )\n",
    ")\n",
    "clinvar.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_clinvar = input_ds.key_by(\"clinvar_variation_id\")\n",
    "input_clinvar = input_clinvar.annotate(\n",
    "    clinvar=clinvar.key_by(\"rsid\")[input_clinvar.clinvar_variation_id]\n",
    ")\n",
    "\n",
    "print(\"Number of partitions originally: {}\".format(input_clinvar.n_partitions()))\n",
    "input_clinvar = input_clinvar.key_by(input_clinvar.clinvar.locus, input_clinvar.clinvar.alleles)\n",
    "input_clinvar = input_clinvar.repartition(64, shuffle=True)\n",
    "input_clinvar = input_clinvar.checkpoint(\"hail-checkpoints/input_clinvar.ht\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import time\n",
    "# print(\"Persisting clinvar\")\n",
    "# start = time.time()\n",
    "# clinvar_persistent = clinvar.persist()\n",
    "# # gnomad_persistent = gnomad.persist()\n",
    "# end = time.time()\n",
    "# print(\"Persisting took {} seconds\".format((end - start)))\n",
    "\n",
    "\n",
    "# print(\"Checkpointing clinvar\")\n",
    "# start = time.time()\n",
    "# clinvar_persistent = clinvar_persistent.checkpoint(\"hail-checkpoints/clinvar.ht\", overwrite=True)\n",
    "# end = time.time()\n",
    "# print(\"Checkpointing took {} seconds\".format((end - start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnomad_persistent.describe()\n",
    "# gnomad_filtered = gnomad_persistent.filter(\n",
    "# #     clinvar.any(clinvar.locus == gnomad_persistent.locus & clinvar.alleles = gnomad_persistent.alleles)\n",
    "#     hl.is_defined(clinvar_persistent[gnomad_persistent.locus, gnomad_persistent.alleles])\n",
    "# )\n",
    "\n",
    "# gnomad_filtered_count = gnomad_filtered.count()\n",
    "# print(\"Gnomad filtered to {} rows\".format(gnomad_filtered_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Combine gnomad and clinvar\n",
    "print(\"Left joining gnomad to input_clinvar\")\n",
    "# start = time.time()\n",
    "# gnomad_clinvar = gnomad_filtered.join(clinvar_persistent, how=\"right\") # take only those from clinvar\n",
    "input_clinvar_gnomad = input_clinvar.annotate(\n",
    "    gnomad=gnomad[input_clinvar.clinvar.locus, input_clinvar.clinvar.alleles]\n",
    ")\n",
    "# count = gnomad_clinvar.count()\n",
    "# end = time.time()\n",
    "# print(\"Counting took {} seconds\".format((end - start)))\n",
    "# print(\"Gnomad/Clinvar combined to {} rows\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_annotated = input_ds.key_by(\"clinvar_variation_id\")\n",
    "# clinvar_by_id = clinvar_persistent.key_by(\"rsid\").persist()\n",
    "# input_annotated = input_annotated.annotate(\n",
    "#     clinvar=clinvar_by_id[input_annotated.clinvar_variation_id]\n",
    "# )\n",
    "\n",
    "# input_annotated = input_annotated.annotate(\n",
    "#     gnomad=gnomad_filtered[input_annotated.clinvar.locus, input_annotated.clinvar.alleles]\n",
    "# )\n",
    "\n",
    "# input_annotated = input_annotated.annotate(\n",
    "#     locus=input_annotated.clinvar.locus,\n",
    "#     alleles=input_annotated.clinvar.alleles\n",
    "# )\n",
    "\n",
    "# input_annotated = input_annotated.key_by(\"locus\", \"alleles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END MODIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# print(\"Persisting gnomad_clinvar to memory and disk\")\n",
    "# start = time.time()\n",
    "# gnomad_clinvar_persisted = gnomad_clinvar.persist()\n",
    "# end = time.time()\n",
    "# print(\"Persisting gnomad_clinvar took {} seconds\".format((end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter gnomad to records in input_ds\n",
    "# input_clinvar = input_ds.annotate(\n",
    "#     clinvar=clinvar.key_by(clinvar.rsid)[input_ds.clinvar_variation_id]\n",
    "# )\n",
    "# input_clinvar = input_clinvar.key_by(\n",
    "#     input_clinvar.clinvar.locus, input_clinvar.clinvar.alleles\n",
    "# )\n",
    "# input_clinvar.describe()\n",
    "# input_clinvar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_popmax = input_clinvar_gnomad\n",
    "\n",
    "with_popmax.describe()\n",
    "# Add freq + faf global annotations from gnomad to new table\n",
    "with_popmax = with_popmax.annotate_globals(freq_index_dict=hl.eval(gnomad.globals.freq_index_dict))\n",
    "with_popmax = with_popmax.annotate_globals(faf_index_dict=hl.eval(gnomad.globals.faf_index_dict))\n",
    "# Pull gnomad faf and freq to top level\n",
    "with_popmax = with_popmax.annotate(faf=with_popmax.gnomad.faf)\n",
    "with_popmax = with_popmax.annotate(freq=with_popmax.gnomad.freq)\n",
    "\n",
    "# with_popmax = with_popmax.annotate_globals(freq_index_dict=)\n",
    "\n",
    "with_popmax.describe()\n",
    "\n",
    "# Compute popmax and annotate\n",
    "with_popmax = add_popmax_af(with_popmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_popmax.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(with_popmax.row.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_popmax.key.collect()\n",
    "input_table_columns = list(input_ds.row.keys())\n",
    "print(input_table_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ds = with_popmax\n",
    "# Query\tCA ID\tClinVar Name\tClinVar ID\t37 Coorindates\t38 Coordinates\tGene\tNM\tNP\tclinvar_variation_id\n",
    "output_ds = output_ds.annotate(\n",
    "    # already keyed by clinvar_variation_id\n",
    "#     clinvar_variation_id=output_ds.clinvar_variation_id,\n",
    "    clinvar_review_status=hl.delimit(output_ds.clinvar.info[\"CLNREVSTAT\"], \",\"),\n",
    "    clinvar_significance=hl.delimit(output_ds.clinvar.info[\"CLNSIG\"], \",\"),\n",
    "    clinvar_significance_interpretations=hl.delimit(output_ds.clinvar.info[\"CLNSIGCONF\"], \",\"),\n",
    "    # Hail parses the CLNDN (and related like CLNDNINCL) incorrectly\n",
    "    # Since ',' is allowed in condition names, ClinVar uses '|' to separate them\n",
    "    # But Hail separates into an array based on ',' instead of '|'\n",
    "    # If we re-join the string with ',' it will match that from ClinVar\n",
    "    clinvar_conditions=hl.delimit(output_ds.clinvar.info[\"CLNDN\"], \",\"),\n",
    "#     popmax_pop = output_ds.popmax_faf.meta[\"pop\"],\n",
    "    popmax_pop = output_ds.popmax_index_dict_key,\n",
    "    popmax_ac = output_ds.popmax_faf_pop_freq.AC,\n",
    "    popmax_an = output_ds.popmax_faf_pop_freq.AN,\n",
    ")\n",
    "output_ds = output_ds.key_by(\"locus\", \"alleles\")\n",
    "output_ds = output_ds.select(\n",
    "    *(input_table_columns + \n",
    "      ['clinvar_review_status', \n",
    "      'clinvar_significance',\n",
    "      'clinvar_significance_interpretations',\n",
    "      'clinvar_conditions',\n",
    "      'popmax_pop',\n",
    "      'popmax_ac',\n",
    "      'popmax_an'])\n",
    ")\n",
    "output_ds.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output table number of partitions: {}\".format(output_ds.n_partitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select desired output fields (columns are ordered as provided)\n",
    "# output_ds = with_popmax\n",
    "# # Query\tCA ID\tClinVar Name\tClinVar ID\t37 Coorindates\t38 Coordinates\tGene\tNM\tNP\tclinvar_variation_id\n",
    "# output_ds = output_ds.select(\n",
    "#     # already keyed by clinvar_variation_id\n",
    "#     clinvar_review_status=hl.delimit(output_ds.clinvar.info[\"CLNREVSTAT\"], \",\"),\n",
    "#     clinvar_significance=hl.delimit(output_ds.clinvar.info[\"CLNSIG\"], \",\"),\n",
    "#     clinvar_significance_interpretations=hl.delimit(output_ds.clinvar.info[\"CLNSIGCONF\"], \",\"),\n",
    "#     # Hail parses the CLNDN (and related like CLNDNINCL) incorrectly\n",
    "#     # Since ',' is allowed in condition names, ClinVar uses '|' to separate them\n",
    "#     # But Hail separates into an array based on ',' instead of '|'\n",
    "#     # If we re-join the string with ',' it will match that from ClinVar\n",
    "#     clinvar_conditions=hl.delimit(output_ds.clinvar.info[\"CLNDN\"], \",\"),\n",
    "    \n",
    "#     # gnomad popmax things\n",
    "#     popmax_pop = output_ds.popmax_faf.meta[\"pop\"],\n",
    "#     popmax_ac = output_ds.popmax_faf_pop_freq.AC,\n",
    "#     popmax_an = output_ds.popmax_faf_pop_freq.AN,\n",
    "# )\n",
    "\n",
    "# output_ds = output_ds.order_by(\n",
    "#     hl.int(output_ds.clinvar_variation_id) # Assume all clinvar variation ids are integers\n",
    "# )\n",
    "\n",
    "# output_ds.describe()\n",
    "\n",
    "# Export to TSV\n",
    "import time\n",
    "print(\"Starting export to %s\" % report_filename)\n",
    "start_time = time.time()\n",
    "output_ds.export(report_filename)\n",
    "end_time = time.time()\n",
    "print(\"Export took %.2f seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The export is in HDFS now, copy to machine-local file\n",
    "report_localpath = os.path.join(os.getcwd(), report_filename)\n",
    "hdfs_to_local(report_filename, report_localpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to bucket and filepath set at top of notebook\n",
    "print(\"Uploading {} bytes to GCS\".format(os.path.getsize(report_localpath)))\n",
    "local_to_bucket(report_localpath, report_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
