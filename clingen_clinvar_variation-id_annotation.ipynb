{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output bucket to write to, dataproc service account must have write access\n",
    "# Do not include trailing slash or \"gs://\"\n",
    "output_bucket = \"clingen-dataproc-workspace-kferrite\"\n",
    "# Set the TSV path to write into bucket. Can contain slash like \"folder/file.tsv\"\n",
    "# Do not include leading slash\n",
    "report_filename = \"clinvar-annotation.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "# `idempontent=True` is useful for running all cells in the notebook\n",
    "hl.init(idempotent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions for file placement\n",
    "import subprocess\n",
    "\n",
    "def run_args(args, fail_on_stderr=False, success_codes=[0]) -> tuple: # (stdout,stderr,returncode)\n",
    "    print(args)\n",
    "    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "    if (fail_on_stderr and len(stderr) > 0) or (p.returncode not in success_codes):\n",
    "        raise RuntimeError(\"command {} failed with code {}:{}\".format(\n",
    "            args, p.returncode, stderr))\n",
    "    return (stdout, stderr, p.returncode)\n",
    "\n",
    "def local_to_bucket(local_path:str, gcs_path:str):\n",
    "    if not gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = \"gs://{}/{}\".format(output_bucket, gcs_path)\n",
    "    args = [\"gsutil\", \"cp\", local_path, gcs_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def bucket_to_local(gcs_path:str, local_path:str):\n",
    "    if not gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = \"gs://{}/{}\".format(output_bucket, gcs_path)\n",
    "    args = [\"gsutil\", \"cp\", gcs_path, local_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def local_to_hdfs(local_path:str, hdfs_path:str):\n",
    "    args = [\"hdfs\", \"dfs\", \"-rm\", hdfs_path]\n",
    "    run_args(args, success_codes=[0,1]) # Allow error\n",
    "    args = [\"hdfs\", \"dfs\", \"-cp\", \"file://\" + local_path, hdfs_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def hdfs_to_local(hdfs_path:str, local_path:str):\n",
    "    if os.path.exists(local_path):\n",
    "        os.remove(local_path)\n",
    "    args = [\"hdfs\", \"dfs\", \"-cp\", hdfs_path, \"file://\" + local_path]\n",
    "    run_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain desired thresholds\n",
    "import io, re\n",
    "\n",
    "input_filename = \"input_files/clinvar_variation_ids.txt\"\n",
    "bucket_to_local(input_filename, input_filename)\n",
    "with open(input_filename) as f_in:\n",
    "    variation_ids = [line.strip() for line in f_in if len(line) > 0]\n",
    "print(\"Loaded {} variation ids\".format(len(variation_ids)))\n",
    "print(variation_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import ClinVar VCF as Hail Table\n",
    "# clinvar = hl.import_vcf(\"/path/to/clinvar.vcf.gz\", force_bgz=True, drop_samples=True, skip_invalid_loci=True).rows()\n",
    "\n",
    "# Download clinvar BGZF\n",
    "import os, requests, subprocess\n",
    "\n",
    "# Function to download a file to a localpath. ClinVar VCF is small enough to download to dataproc default local disk.\n",
    "def download_to_file(url, filepath):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(\"Failed to obtain ClinVar VCF:{}\\n{}\".format(r.status_code))\n",
    "    with open(filepath, \"wb\") as fout: \n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "             if chunk:\n",
    "                 fout.write(chunk)\n",
    "# This url always points to the latest dump file, updated periodically by ClinVar\n",
    "clinvar_vcf_url = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz\"\n",
    "clinvar_vcf_localpath = \"/home/hail/clinvar.vcf.gz\"\n",
    "clinvar_vcf_hdfs = \"clinvar.vcf.gz\"\n",
    "download_to_file(clinvar_vcf_url, clinvar_vcf_localpath)\n",
    "assert(os.path.exists(clinvar_vcf_localpath))\n",
    "print(\"Downloaded ClinVar VCF, file size (expecting ~30M): %d\" % os.path.getsize(clinvar_vcf_localpath))\n",
    "\n",
    "# Hail needs the file in HDFS\n",
    "local_to_hdfs(clinvar_vcf_localpath, clinvar_vcf_hdfs)\n",
    "# p = subprocess.Popen([\"hdfs\", \"dfs\", \"-cp\", \"file://\" + clinvar_vcf_localpath, clinvar_vcf_hdfs])\n",
    "# print(p.communicate())\n",
    "\n",
    "clinvar = hl.import_vcf(\n",
    "    clinvar_vcf_hdfs,\n",
    "    force_bgz=True,\n",
    "    drop_samples=True, \n",
    "    skip_invalid_loci=True\n",
    ").rows()\n",
    "print(\"Imported {} records from ClinVar\".format(clinvar.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clinvar.describe()\n",
    "# clinvar.show()\n",
    "\n",
    "# Filter to input set\n",
    "variation_ids_hl = hl.literal(variation_ids)\n",
    "clinvar_filtered = clinvar.filter(\n",
    "    variation_ids_hl.contains(clinvar.rsid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find any ids in input that don't exist in table\n",
    "\n",
    "# up to megabytes in size\n",
    "clinvar_ids = [\n",
    "    rec.rsid for rec in clinvar_filtered.select(clinvar_filtered.rsid).collect()\n",
    "]\n",
    "# print(clinvar_ids)\n",
    "\n",
    "missing_ids = [i for i in variation_ids if i not in clinvar_ids]\n",
    "print(\"Missing:\\n\" + \"\\n\".join(missing_ids))\n",
    "\n",
    "duplicate_ids = []\n",
    "id_counts = {}\n",
    "for i in variation_ids:\n",
    "    if i not in id_counts:\n",
    "        id_counts[i] = 0\n",
    "    id_counts[i] += 1\n",
    "print(\"Duplicates:\")\n",
    "for k,v in id_counts.items():\n",
    "    if v > 1:\n",
    "        print(\"{}, count={}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select desired output fields (columns are ordered as provided)\n",
    "output_ds = clinvar_filtered\n",
    "\n",
    "output_ds = output_ds.select(\n",
    "    clinvar_variation_id=output_ds.rsid,\n",
    "    clinvar_review_status=hl.delimit(output_ds.info[\"CLNREVSTAT\"], \",\"),\n",
    "    clinvar_significance=hl.delimit(output_ds.info[\"CLNSIG\"], \",\"),\n",
    "    clinvar_significance_interpretations=hl.delimit(output_ds.info[\"CLNSIGCONF\"], \",\"),\n",
    "    # Hail parses the CLNDN (and related like CLNDNINCL) incorrectly\n",
    "    # Since ',' is allowed in condition names, ClinVar uses '|' to separate them\n",
    "    # But Hail separates into an array based on ',' instead of '|'\n",
    "    # If we re-join the string with ',' it will match that from ClinVar\n",
    "    clinvar_conditions=hl.delimit(output_ds.info[\"CLNDN\"], \",\") \n",
    ")\n",
    "output_ds = output_ds.order_by(\n",
    "    hl.int(output_ds.clinvar_variation_id) # Assume all clinvar variation ids are integers\n",
    ")\n",
    "\n",
    "# output_ds.describe()\n",
    "\n",
    "# Export to TSV\n",
    "import time\n",
    "print(\"Starting export to %s\" % report_filename)\n",
    "start_time = time.time()\n",
    "output_ds.export(report_filename)\n",
    "end_time = time.time()\n",
    "print(\"Export took %.2f seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The export is in HDFS now, copy to machine-local file\n",
    "report_localpath = os.path.join(os.getcwd(), report_filename)\n",
    "hdfs_to_local(report_filename, report_localpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to bucket and filepath set at top of notebook\n",
    "print(\"Uploading {} bytes to GCS\".format(os.path.getsize(report_localpath)))\n",
    "local_to_bucket(report_localpath, report_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}