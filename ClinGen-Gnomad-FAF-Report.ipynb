{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output bucket to write to, dataproc service account must have write access\n",
    "# Do not include trailing slash or \"gs://\"\n",
    "output_bucket = \"clingen-dataproc-workspace-kferrite\"\n",
    "# Set the TSV path to write into bucket. Can contain slash like \"folder/file.tsv\"\n",
    "# Do not include leading slash\n",
    "output_filename = \"report.tsv\"\n",
    "\n",
    "\n",
    "# Set this to true to limit output variants to be those within transcript coding regions\n",
    "transcript_filter = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "# `idempontent=True` is useful for running all cells in the notebook\n",
    "hl.init(idempotent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain desired thresholds\n",
    "import io, re\n",
    "\n",
    "thresholds = \"\"\"\n",
    "MYH7 BA1 0.10%\n",
    "MYH7 BS1 0.02%\n",
    "PTPN11 BA1 0.05%\n",
    "PTPN11 BS1 0.03%\n",
    "CDH1 BA1 0.20%\n",
    "CDH1 BS1 0.10%\n",
    "RUNX1 BA1 0.15%\n",
    "RUNX1 BS1 0.015%\n",
    "TP53 BA1 0.10%\n",
    "TP53 BS1 0.03%\n",
    "GJB2 BA1 0.50%\n",
    "GJB2 BS1 0.30%\n",
    "PAH BA1 1.50%\n",
    "PAH BS1 0.20%\n",
    "GAA BA1 1%\n",
    "GAA BS1 0.50%\n",
    "HRAS BA1 0.05%\n",
    "HRAS BS1 0.03%\n",
    "NRAS BA1 0.05%\n",
    "NRAS BS1 0.03%\n",
    "\"\"\"\n",
    "\n",
    "thresh_reader = io.StringIO(thresholds)\n",
    "\n",
    "def parse_thresholds(reader):\n",
    "    \"\"\"\n",
    "    Expects `reader` to be a file/io reader \n",
    "    with a newline delimited list of:\n",
    "    <gene-symbol> <thresh-name> <thresh-percent>\n",
    "    ...\n",
    "    <thresh-percent> may be pure float or contain % denoting 10e2 scaling\n",
    "    Returns a multilayer dictionary of gene(str)->threshname(str)->AF->percent(float)\n",
    "    Example:\n",
    "    gene_thresholds = {\n",
    "        \"MYH7\": {\n",
    "            \"BA1\": {\n",
    "                \"AF\": 0.0005\n",
    "            },\n",
    "            \"BS1\": {\n",
    "                \"AF\": 0.0002\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    thresholds = {}\n",
    "    # Load whole reader contents, should be small enough\n",
    "    contents = reader.read()\n",
    "    lines = contents.splitlines()\n",
    "    lines = [l for l in lines if l and len(l)] # skip empty lines\n",
    "    for line in lines:\n",
    "        gene, thresh_name, thresh = re.split(\"\\s+\", line)\n",
    "        if thresh.endswith(\"%\"):\n",
    "            thresh = float(thresh[:-1]) / 100.0\n",
    "        else:\n",
    "            thresh = float(thresh)\n",
    "        if gene not in thresholds:\n",
    "            thresholds[gene] = {}\n",
    "        thresholds[gene][thresh_name] = {\"AF\": thresh}\n",
    "    return thresholds\n",
    "\n",
    "        \n",
    "gene_thresholds = parse_thresholds(thresh_reader)\n",
    "print(gene_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "# Read gnomAD data as Hail Tables\n",
    "# sources = {\n",
    "#     \"Exomes\": \"gs://gnomad-public/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\",\n",
    "#     \"Genomes\": \"gs://gnomad-public/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\"\n",
    "# }\n",
    "\n",
    "ds_exomes = hl.read_table(\"gs://gnomad-public/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\")\n",
    "ds_exomes = ds_exomes.annotate(\n",
    "    source=\"gnomAD Exomes\"\n",
    ")\n",
    "ds_genomes = hl.read_table(\"gs://gnomad-public/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht\")\n",
    "ds_genomes = ds_genomes.annotate(\n",
    "    source=\"gnomAD Genomes\"\n",
    ")\n",
    "\n",
    "# Can perform a union here if wanting both (ds = ds1.union(ds2))\n",
    "def select_necessary_cols(ds):\n",
    "    ds = ds.select(ds.freq, ds.faf, ds.vep, ds.source)\n",
    "    return ds\n",
    "\n",
    "ds_exomes = select_necessary_cols(ds_exomes)\n",
    "ds_genomes = select_necessary_cols(ds_genomes)\n",
    "\n",
    "ds = ds_genomes.union(ds_exomes, unify=True)\n",
    "\n",
    "# Show the schema of the hail Table\n",
    "# ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ds.freq has raw frequency information, including AN, AC, and pop label. This is an array of \n",
    "structs, at indices determined by the categories in ds.globals.freq_index_dict\n",
    "\n",
    "ds.faf has filtered allele frequency information, including confidence intervals faf95 adn faf99.\n",
    "This is an array of structs, at indices determined by the category map in ds.globals.faf_index_dict\n",
    "\"\"\"\n",
    "\n",
    "def add_popmax_af(ds):\n",
    "    \"\"\"\n",
    "    Adds a popmax_faf and popmax_af_pop column to the ds Hail Table.\n",
    "    \n",
    "    popmax_faf is a faf structure from the original ds, containing the maximum faf of the\n",
    "    listed faf structures in the original ds, based on the filtering criteria \n",
    "    `default_faf_filter_type`. \n",
    "    \n",
    "    The popmax_index_dict_key column contains the text field from the\n",
    "    ds.globals.faf_index_dict which corresponds to each popmax_faf. This is similar to the\n",
    "    ds.popmax_faf.meta[\"pop\"] value but not exactly the same (gnomad_afr vs afr)\n",
    "    \n",
    "    Returns the updated ds.\n",
    "    \"\"\"\n",
    "    # Identify indices in FAF field that correspond to the entire dataset (not a subset like non-cancer)\n",
    "    # faf_index_map = [(k,v) for k, v in hl.eval(ds.globals.faf_index_dict).items() if k.startswith(\"gnomad_\")]\n",
    "    from enum import Enum\n",
    "    class FafFilterType(Enum):\n",
    "        # Each correponds to a filter func for a (k,v) of faf label to value\n",
    "        GNOMAD_GLOBAL = lambda t: t[0] == \"gnomad\"\n",
    "        GNOMAD_SUPERPOP = lambda t: t[0].startswith(\"gnomad_\")\n",
    "        ANY = lambda t: True\n",
    "\n",
    "    # By default, filter to superpopulations aggregate faf\n",
    "    default_faf_filter_type = FafFilterType.GNOMAD_SUPERPOP\n",
    "\n",
    "    def faf_filter(faf_idx_tuple:tuple):\n",
    "        return default_faf_filter_type(faf_idx_tuple)\n",
    "\n",
    "    # Get list of the global faf_index_dict which meets the default_faf_filter criteria\n",
    "    # This gives the indices of the desired populations, by default will take all top level populations\n",
    "    faf_index_map = list(filter(faf_filter, [(k,v) for k,v in hl.eval(ds.globals.faf_index_dict).items()]))\n",
    "    faf_indices = [v for k,v in faf_index_map]\n",
    "    faf_labels = [k for k,v in faf_index_map]\n",
    "    \n",
    "    # Annotate table with popmax FAF\n",
    "    \n",
    "    # This only will return the maximum pop FAF for each\n",
    "    # variant, even if multiple populations meet the criteria. \n",
    "    # If we want all matching populations, need an explode() call\n",
    "    # to flatten the pop FAFs into a record per pop per variant\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "        popmax_faf=hl.sorted(\n",
    "            # Take only the FAF entries that correspond to the desired populations (faf_indices)\n",
    "            hl.literal(faf_indices).map(lambda i: ds.faf[i]),\n",
    "            # Sort by 95% confidence FAF\n",
    "            lambda faf_entry: faf_entry.faf95,\n",
    "            # Sort high to low\n",
    "            reverse=True\n",
    "        )[0] # Take the first entry with the highest FAF\n",
    "        ,\n",
    "        # Label of the freq_index_dict entry for this record's max pop\n",
    "        popmax_index_dict_key=hl.sorted(\n",
    "            # List of tuples of (poplabel, faf_index)\n",
    "            list(zip(list(faf_labels), list(faf_indices))),\n",
    "\n",
    "            # Take only the FAF entries that correspond to the entire dataset\n",
    "            # Sort by 95% confidence FAF\n",
    "            key=lambda tpl: ds.faf[tpl[1]].faf95,\n",
    "            # Sort high to low\n",
    "            reverse=True\n",
    "        )[0][0] # Take the first entry, which has the highest FAF\n",
    "    )\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "#         popmax_faf_pop_freq=ds.freq[ds.globals.freq_index_dict[\"gnomad_\" + ds.popmax_faf.meta.get(\"pop\")]]\n",
    "\n",
    "        # ds.globals.freq_index_dict uses the same keys as ds.globals.faf_index_dict so\n",
    "        # we can reuse ds.popmax_index_dict_key created above\n",
    "        popmax_faf_pop_freq=ds.freq[ds.globals.freq_index_dict[ds.popmax_index_dict_key]] \n",
    "    )\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "ds = add_popmax_af(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These next 2 functions override functions from hail.experimental, modified to return a mapping\n",
    "# of gene_symbols to the intervals they correspond to. Existing methods return unordered list\n",
    "\n",
    "import operator\n",
    "import functools\n",
    "from hail.genetics.reference_genome import reference_genome_type\n",
    "from hail.typecheck import typecheck, nullable, sequenceof\n",
    "from hail.utils.java import info\n",
    "from hail.utils import new_temp_file\n",
    "\n",
    "def _load_gencode_gtf(gtf_file=None, reference_genome=None):\n",
    "    \"\"\"\n",
    "    Get Gencode GTF (from file or reference genome)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_genome : :obj:`str` or :class:`.ReferenceGenome`, optional\n",
    "       Reference genome to use (passed along to import_gtf).\n",
    "    gtf_file : :obj:`str`\n",
    "       GTF file to load. If none is provided, but `reference_genome` is one of\n",
    "       `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :class:`.Table`\n",
    "    \"\"\"\n",
    "    GTFS = {\n",
    "        'GRCh37': 'gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz',\n",
    "        'GRCh38': 'gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz',\n",
    "    }\n",
    "    if reference_genome is None:\n",
    "        reference_genome = hl.default_reference().name\n",
    "    else:\n",
    "        reference_genome = reference_genome.name\n",
    "    if gtf_file is None:\n",
    "        gtf_file = GTFS.get(reference_genome)\n",
    "        if gtf_file is None:\n",
    "            raise ValueError(\n",
    "                'get_gene_intervals requires a GTF file, or the reference genome be one of GRCh37 or GRCh38 (when on Google Cloud Platform)')\n",
    "    ht = hl.experimental.import_gtf(gtf_file, reference_genome=reference_genome,\n",
    "                                    skip_invalid_contigs=True, min_partitions=12)\n",
    "    ht = ht.annotate(gene_id=ht.gene_id.split('\\\\.')[0],\n",
    "                     transcript_id=ht.transcript_id.split('\\\\.')[0])\n",
    "    return ht\n",
    "\n",
    "@typecheck(gene_symbols=nullable(sequenceof(str)),\n",
    "           gene_ids=nullable(sequenceof(str)),\n",
    "           transcript_ids=nullable(sequenceof(str)),\n",
    "           verbose=bool, reference_genome=nullable(reference_genome_type), gtf_file=nullable(str))\n",
    "def get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None,\n",
    "                       verbose=True, reference_genome=None, gtf_file=None):\n",
    "    \"\"\"Get intervals of genes or transcripts.\n",
    "\n",
    "    Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.\n",
    "\n",
    "    On Google Cloud platform:\n",
    "    Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz\n",
    "    Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37'))  # doctest: +SKIP\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    gene_symbols : :obj:`list` of :obj:`str`, optional\n",
    "       Gene symbols (e.g. PCSK9).\n",
    "    gene_ids : :obj:`list` of :obj:`str`, optional\n",
    "       Gene IDs (e.g. ENSG00000223972).\n",
    "    transcript_ids : :obj:`list` of :obj:`str`, optional\n",
    "       Transcript IDs (e.g. ENSG00000223972).\n",
    "    verbose : :obj:`bool`\n",
    "       If ``True``, print which genes and transcripts were matched in the GTF file.\n",
    "    reference_genome : :obj:`str` or :class:`.ReferenceGenome`, optional\n",
    "       Reference genome to use (passed along to import_gtf).\n",
    "    gtf_file : :obj:`str`\n",
    "       GTF file to load. If none is provided, but `reference_genome` is one of\n",
    "       `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :obj:`list` of :class:`.Interval`\n",
    "    \"\"\"\n",
    "    if gene_symbols is None and gene_ids is None and transcript_ids is None:\n",
    "        raise ValueError('get_gene_intervals requires at least one of gene_symbols, gene_ids, or transcript_ids')\n",
    "    ht = _load_gencode_gtf(gtf_file, reference_genome)\n",
    "    criteria = []\n",
    "    if gene_symbols:\n",
    "        criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_name == y), gene_symbols))\n",
    "    if gene_ids:\n",
    "        criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\\\.')[0]), gene_ids))\n",
    "    if transcript_ids:\n",
    "        criteria.append(hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\\\.')[0]), transcript_ids))\n",
    "\n",
    "    ht = ht.filter(functools.reduce(operator.ior, criteria))\n",
    "    gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interval)))\n",
    "    if verbose:\n",
    "        info(f'get_gene_intervals found {len(gene_info)} entries:\\n'\n",
    "             + \"\\n\".join(map(lambda x: f'{x[0]}: {x[1]} ({x[2] if x[0] == \"gene\" else x[3]})', gene_info)))\n",
    "    # intervals = list(map(lambda x: x[-1], gene_info))\n",
    "    intervals = list(map(lambda x: {\n",
    "        'gene_symbol': x[1],\n",
    "        'gene_id': x[2],\n",
    "        'transcript_id': x[3],\n",
    "        'interval': x[4]\n",
    "    }, gene_info))\n",
    "    return intervals\n",
    "\n",
    "                         \n",
    "# Look up intervals for the gene symbols in the input thresholds\n",
    "gene_symbols = [k for k in gene_thresholds.keys()]\n",
    "intervals = get_gene_intervals(gene_symbols=gene_symbols, reference_genome=\"GRCh37\")\n",
    "\n",
    "def get_gene_interval(gene_symbol:str):\n",
    "    global intervals\n",
    "    for i in intervals:\n",
    "        if i[\"gene_symbol\"] == gene_symbol:\n",
    "            return i[\"interval\"]\n",
    "    print(\"Getting new gene interval: %s\" % gene_symbol)\n",
    "    i = get_gene_intervals(gene_symbols=[gene_symbol], reference_genome=\"GRCh37\")[0]\n",
    "    intervals.append(i)\n",
    "    return i[\"interval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some preliminary annotations\n",
    "ds_crit = ds\n",
    "\n",
    "# This was removed because we can't assume all gene symbols are the same, a variant can have >1\n",
    "# ds_crit = ds_http://localhost:8123/notebooks/clingen-dataproc-workspace-kferrite/ClinGen-Gnomad-FAF-Report-V2.ipynb#crit.annotate(\n",
    "#     gene_symbol=ds_crit.vep.transcript_consequences.gene_symbol # Can't assume they are all the same\n",
    "# )\n",
    "\n",
    "print(intervals)\n",
    "ivl_struct_list = hl.literal(\n",
    "    [hl.struct(\n",
    "        gene_symbol=i[\"gene_symbol\"],\n",
    "        gene_id=i[\"gene_id\"],\n",
    "        transcript_id=i[\"transcript_id\"],\n",
    "        interval=i[\"interval\"]\n",
    "    ) for i in intervals]\n",
    ")\n",
    "\n",
    "# Filter by intervals of genes provided in input criteria\n",
    "ds_crit = hl.filter_intervals(ds_crit, [i[\"interval\"] for i in intervals])\n",
    "\n",
    "# Now attach the gene field using 1 of two methods.\n",
    "# If transcript_filter is true, attach gene label based on transcript_consequences\n",
    "# If transcript_filter is false, attach based on which gene interval it is contained in\n",
    "if transcript_filter is False:\n",
    "    ds_crit = ds_crit.annotate(\n",
    "#         gene=lambda _: \n",
    "        gene=ivl_struct_list.find(\n",
    "            lambda ivl: ivl[\"interval\"].contains(ds_crit.locus)\n",
    "        ).gene_symbol\n",
    "    )\n",
    "else:\n",
    "    # Explode a new record per transcript consequence, each now has 1 gene\n",
    "    ds_crit = ds_crit.annotate(\n",
    "        transcript_consequences=ds_crit.vep.transcript_consequences\n",
    "    )\n",
    "    ds_crit = ds_crit.explode(\"transcript_consequences\")\n",
    "    ds_crit = ds_crit.annotate(\n",
    "        gene=ds_crit.transcript_consequences.gene_symbol\n",
    "    )\n",
    "\n",
    "\n",
    "# Sort each gene's criteria thresholds descending by AF so first hl.find is the max\n",
    "gene_thresholds = hl.literal(gene_thresholds)\n",
    "gene_thresholds_sorted = gene_thresholds.map_values(\n",
    "    lambda gene_criteria: hl.sorted(\n",
    "        # Transform {\"BA1\": {\"AF\": 0.02}} to list of [(\"BA1\", {\"AF\": 0.02})]\n",
    "        gene_criteria.keys().map(lambda crit_name: (crit_name, gene_criteria[crit_name])),\n",
    "        \n",
    "        # Key to sort the above ArrayExpression\n",
    "        lambda t: t[1][\"AF\"],\n",
    "        \n",
    "        # Reverse order so we find the max threshold first\n",
    "        reverse=True\n",
    "    )\n",
    ")\n",
    "print(gene_thresholds_sorted.collect())\n",
    "\n",
    "# Filter to variants in genes we care about\n",
    "ds_crit = ds_crit.filter(\n",
    "    gene_thresholds.keys().contains(ds_crit.gene)\n",
    ")\n",
    "\n",
    "ds_crit = ds_crit.annotate(\n",
    "    # Get the max AF threshold which is less or equal to popmax_faf.faf95\n",
    "    criteria_satisfied=hl.or_missing(\n",
    "        # Condition\n",
    "        gene_thresholds.keys().any(\n",
    "            lambda threshold_gene: ds_crit.gene == threshold_gene\n",
    "        ),\n",
    "        \n",
    "        # If this gene is in criteria, find max criteria (already reverse sorted, find gets first)\n",
    "        hl.find(\n",
    "            lambda tpl: tpl[1][\"AF\"] <= ds_crit.popmax_faf.faf95,\n",
    "            # gene_thresholds[ds_crit.gene][crit_name][\"AF\"] <= ds_crit.popmax_faf.faf95,\n",
    "\n",
    "            # List of (crit_name, {\"AF\": 0.02})\n",
    "            gene_thresholds_sorted[ds_crit.gene]\n",
    "        )[0] # [0] returns the criteria name (ex: BA1)\n",
    "        #[1][\"AF\"]\n",
    "    ),\n",
    "#     gene_symbol=hl.join(\n",
    "#         \",\",\n",
    "#         # Join the gene symbols of the record which are in gene_thresholds, ignore others\n",
    "#         ds_crit.vep.transcript_consequences.gene_symbol.filter(\n",
    "#             lambda s: gene_thresholds.keys().contains(s)\n",
    "#         )\n",
    "#     )\n",
    ")\n",
    "\n",
    "# Filter to variants which meet a criteria\n",
    "ds_crit = ds_crit.filter(\n",
    "    ~hl.is_missing(ds_crit.criteria_satisfied)\n",
    ")\n",
    "\n",
    "\n",
    "filtered_ds = ds_crit.select(\n",
    "    criteria_satisfied = ds_crit.criteria_satisfied,\n",
    "    source = ds_crit.source,\n",
    "    gene = ds_crit.gene,\n",
    "    popmax_pop = ds_crit.popmax_faf.meta[\"pop\"],\n",
    "    popmax_ac = ds_crit.popmax_faf_pop_freq.AC,\n",
    "    popmax_an = ds_crit.popmax_faf_pop_freq.AN,\n",
    "    faf95 = ds_crit.popmax_faf.faf95,\n",
    "    genomic_coordinates = hl.format(\"%s-%s-%s-%s\",\n",
    "        ds_crit.locus.contig,\n",
    "        hl.str(ds_crit.locus.position),\n",
    "        ds_crit.alleles[0],\n",
    "        ds_crit.alleles[1]\n",
    "    )\n",
    ")\n",
    "# filtered_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import ClinVar VCF as Hail Table\n",
    "# clinvar = hl.import_vcf(\"/path/to/clinvar.vcf.gz\", force_bgz=True, drop_samples=True, skip_invalid_loci=True).rows()\n",
    "\n",
    "# Download clinvar BGZF\n",
    "import os, requests, subprocess\n",
    "\n",
    "# Function to download a file to a localpath. ClinVar VCF is small enough to download to dataproc default local disk.\n",
    "def download_to_file(url, filepath):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(filepath, \"wb\") as fout: \n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "             if chunk:\n",
    "                 fout.write(chunk)\n",
    "# This url always points to the latest dump file, updated periodically by ClinVar\n",
    "clinvar_vcf_url = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/clinvar.vcf.gz\"\n",
    "clinvar_vcf_localpath = \"/home/hail/clinvar.vcf.gz\"\n",
    "clinvar_vcf_hdfs = \"clinvar.vcf.gz\"\n",
    "download_to_file(clinvar_vcf_url, clinvar_vcf_localpath)\n",
    "assert(os.path.exists(clinvar_vcf_localpath))\n",
    "print(\"Downloaded ClinVar VCF, file size (expecting ~28M): %d\" % os.path.getsize(clinvar_vcf_localpath))\n",
    "\n",
    "# Hail needs the file in HDFS\n",
    "p = subprocess.Popen([\"hdfs\", \"dfs\", \"-cp\", \"file://\" + clinvar_vcf_localpath, clinvar_vcf_hdfs])\n",
    "print(p.communicate())\n",
    "\n",
    "\n",
    "clinvar = hl.import_vcf(\n",
    "    clinvar_vcf_hdfs,\n",
    "    force_bgz=True,\n",
    "    drop_samples=True, \n",
    "    skip_invalid_loci=True\n",
    ").rows()\n",
    "print(clinvar.count())\n",
    "\n",
    "# Join ClinVar table to gnomAD table. ClinVar fields available under the table.clinvar struct\n",
    "gnomad_clinvar_ds = filtered_ds.annotate(\n",
    "    clinvar=clinvar[filtered_ds.locus, filtered_ds.alleles]\n",
    ")\n",
    "\n",
    "# ClinVar VCF export sets ID column to the ClinVar Variation ID (not rsid)\n",
    "# And sets the RS field of INFO to the rsid if it exists.\n",
    "# (https://ftp.ncbi.nlm.nih.gov/pub/clinvar/README_VCF.txt)\n",
    "# Hail then sets this ClinVar ID as the rsid column of the clinvar struct\n",
    "# We can filter to only the variants that exist in clinvar with:\n",
    "# gnomad_clinvar_ds = gnomad_clinvar_ds.filter(\n",
    "#     ~hl.is_missing(gnomad_clinvar_ds.clinvar_rsid)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnomad_clinvar_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select desired output fields (ordered)\n",
    "output_ds = gnomad_clinvar_ds.select(\n",
    "    gnomad_clinvar_ds.criteria_satisfied,\n",
    "    gnomad_clinvar_ds.gene,\n",
    "    gnomad_clinvar_ds.faf95,\n",
    "    gnomad_clinvar_ds.source,\n",
    "    gnomad_clinvar_ds.popmax_pop,\n",
    "    gnomad_clinvar_ds.popmax_ac,\n",
    "    gnomad_clinvar_ds.popmax_an,\n",
    "    gnomad_clinvar_ds.genomic_coordinates,\n",
    "    clinvar_variation_id=gnomad_clinvar_ds.clinvar.rsid,\n",
    "    clinvar_review_status=hl.delimit(gnomad_clinvar_ds.clinvar.info[\"CLNREVSTAT\"], \",\"),\n",
    "    clinvar_significance=hl.delimit(gnomad_clinvar_ds.clinvar.info[\"CLNSIG\"], \",\"),\n",
    "    clinvar_significance_interpretations=hl.delimit(gnomad_clinvar_ds.clinvar.info[\"CLNSIGCONF\"], \",\")\n",
    ")\n",
    "\n",
    "# output_ds.describe()\n",
    "\n",
    "# Export to TSV\n",
    "report_filename = \"report.tsv\"\n",
    "import time\n",
    "print(\"Starting export to %s\" % report_filename)\n",
    "start_time = time.time()\n",
    "output_ds.export(report_filename)\n",
    "end_time = time.time()\n",
    "print(\"Export took %.2f seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The export is in HDFS now, copy to machine-local file\n",
    "report_localpath = os.path.join(os.getcwd(), report_filename)\n",
    "os.system(\"rm %s\" % report_localpath)\n",
    "p = subprocess.Popen([\"hdfs\", \"dfs\", \"-cp\", report_filename, \"file://\" + report_localpath],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(p.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to bucket and filepath set at top of notebook\n",
    "gs_output_file = \"gs://%s/%s\" % (output_bucket, output_filename)\n",
    "p = subprocess.Popen([\"gsutil\", \"cp\", report_localpath, gs_output_file],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(p.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
