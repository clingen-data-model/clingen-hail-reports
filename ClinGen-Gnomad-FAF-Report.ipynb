{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output bucket to write to, dataproc service account must have write access\n",
    "# Do not include trailing slash or \"gs://\"\n",
    "output_bucket = \"clingen-dataproc-workspace-kferrite\"\n",
    "# Set the TSV path to write into bucket. Can contain slash like \"folder/file.tsv\"\n",
    "# Do not include leading slash\n",
    "output_filename = \"report.tsv\"\n",
    "\n",
    "reference_genome = \"GRCh37\"\n",
    "\n",
    "# Set this to true to limit output variants to be those within transcript coding regions\n",
    "transcript_filter = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "# `idempontent=True` is useful for running all cells in the notebook\n",
    "hl.init(idempotent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain desired thresholds\n",
    "import io, re\n",
    "\n",
    "thresholds = \"\"\"\n",
    "MYH7\tBA1\t0.10%\tNM_000257.4\n",
    "MYH7\tBS1\t0.02%\tNM_000257.4\n",
    "PTPN11\tBA1\t0.05%\tNM_002834.5\n",
    "PTPN11\tBS1\t0.03%\tNM_002834.5\n",
    "CDH1\tBA1\t0.20%\tNM_004360.5\n",
    "CDH1\tBS1\t0.10%\tNM_004360.5\n",
    "RUNX1\tBA1\t0.15%\tNM_001754.4\n",
    "RUNX1\tBS1\t0.015%\tNM_001754.4\n",
    "TP53\tBA1\t0.10%\tNM_000546.5\n",
    "TP53\tBS1\t0.03%\tNM_000546.5\n",
    "GJB2\tBA1\t0.50%\tNM_004004.6\n",
    "GJB2\tBS1\t0.30%\tNM_004004.6\n",
    "PAH\tBA1\t1.50%\tNM_000277.3\n",
    "PAH\tBS1\t0.20%\tNM_000277.3\n",
    "GAA\tBA1\t1%\tNM_000152.5\n",
    "GAA\tBS1\t0.50%\tNM_000152.5\n",
    "HRAS\tBA1\t0.05%\tNM_005343.4\n",
    "HRAS\tBS1\t0.03%\tNM_005343.4\n",
    "NRAS\tBA1\t0.05%\tNM_002524.5\n",
    "NRAS\tBS1\t0.03%\tNM_002524.5\n",
    "\"\"\"\n",
    "\n",
    "thresh_reader = io.StringIO(thresholds)\n",
    "\n",
    "def parse_thresholds(reader):\n",
    "    \"\"\"\n",
    "    Expects `reader` to be a file/io reader \n",
    "    with a newline delimited list of:\n",
    "    <gene-symbol> <thresh-name> <thresh-percent> [refseq]\n",
    "    ...\n",
    "    <thresh-percent> may be pure float or contain % denoting 10e2 scaling\n",
    "    Returns a multilayer dictionary of gene(str)->threshname(str)->AF->percent(float)\n",
    "    Example:\n",
    "    gene_thresholds = {\n",
    "        \"MYH7\": {\n",
    "            \"BA1\": {\n",
    "                \"AF\": 0.0005\n",
    "            },\n",
    "            \"BS1\": {\n",
    "                \"AF\": 0.0002\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    thresholds = {}\n",
    "    gene_refseqs = {} # Fill in any that appear in input, fill in rest later\n",
    "    \n",
    "    # Load whole reader contents, should be small enough\n",
    "    contents = reader.read()\n",
    "    lines = contents.splitlines()\n",
    "    lines = [l for l in lines if l and len(l)] # skip empty lines\n",
    "    for line in lines:\n",
    "        terms = re.split(\"\\s+\", line)\n",
    "        if len(terms) < 3:\n",
    "            raise RuntimeError(\"Input lines must contain at least 3 items\")\n",
    "        gene = terms[0]\n",
    "        thresh_name = terms[1]\n",
    "        thresh = terms[2]\n",
    "        \n",
    "        # Store refseq for the gene if provided\n",
    "        if len(terms) >= 4:\n",
    "            if gene in gene_refseqs and gene_refseqs[gene] != terms[3]:\n",
    "                raise RuntimeError(\"Gene %s lines did not contain the same refseq\")\n",
    "            gene_refseqs[gene] = terms[3]\n",
    "        \n",
    "        # Parse percentages\n",
    "        if thresh.endswith(\"%\"):\n",
    "            thresh = float(thresh[:-1]) / 100.0\n",
    "        else:\n",
    "            thresh = float(thresh)\n",
    "        \n",
    "        if gene not in thresholds:\n",
    "            thresholds[gene] = {}\n",
    "        thresholds[gene][thresh_name] = {\"AF\": thresh}\n",
    "    return thresholds, gene_refseqs\n",
    "\n",
    "        \n",
    "gene_thresholds, gene_refseqs = parse_thresholds(thresh_reader)\n",
    "\n",
    "print(\"gene_thresholds:\\n%s\" % gene_thresholds)\n",
    "print(\"input refseqs:\\n%s\" % gene_refseqs)\n",
    "\n",
    "gene_thresholds_exp = hl.literal(gene_thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "# Read gnomAD data as Hail Tables\n",
    "# ds_exomes = hl.read_table(\"gs://gnomad-public/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\")\n",
    "ds_exomes = hl.read_table(\n",
    "    \"gs://gnomad-public-requester-pays/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\")\n",
    "\n",
    "ds_exomes = ds_exomes.annotate(\n",
    "    source=\"gnomAD Exomes\"\n",
    ")\n",
    "# ds_genomes = hl.read_table(\"gs://gnomad-public/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht\")\n",
    "ds_genomes = hl.read_table(\n",
    "    \"gs://gnomad-public-requester-pays/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht\")\n",
    "ds_genomes = ds_genomes.annotate(\n",
    "    source=\"gnomAD Genomes\"\n",
    ")\n",
    "\n",
    "# Can perform a union here if wanting both (ds = ds1.union(ds2))\n",
    "def select_necessary_cols(ds):\n",
    "    ds = ds.select(ds.freq, ds.faf, ds.vep, ds.source)\n",
    "    return ds\n",
    "\n",
    "ds_exomes = select_necessary_cols(ds_exomes)\n",
    "ds_genomes = select_necessary_cols(ds_genomes)\n",
    "\n",
    "ds = ds_genomes.union(ds_exomes, unify=True)\n",
    "\n",
    "# Show the schema of the hail Table\n",
    "# ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ds.freq has raw frequency information, including AN, AC, and pop label. This is an array of \n",
    "structs, at indices determined by the categories in ds.globals.freq_index_dict\n",
    "\n",
    "ds.faf has filtered allele frequency information, including confidence intervals faf95 adn faf99.\n",
    "This is an array of structs, at indices determined by the category map in ds.globals.faf_index_dict\n",
    "\"\"\"\n",
    "\n",
    "def add_popmax_af(ds):\n",
    "    \"\"\"\n",
    "    Adds a popmax_faf and popmax_af_pop column to the ds Hail Table.\n",
    "    \n",
    "    popmax_faf is a faf structure from the original ds, containing the maximum faf of the\n",
    "    listed faf structures in the original ds, based on the filtering criteria \n",
    "    `default_faf_filter_type`. \n",
    "    \n",
    "    The popmax_index_dict_key column contains the text field from the\n",
    "    ds.globals.faf_index_dict which corresponds to each popmax_faf. This is similar to the\n",
    "    ds.popmax_faf.meta[\"pop\"] value but not exactly the same (gnomad_afr vs afr)\n",
    "    \n",
    "    Returns the updated ds.\n",
    "    \"\"\"\n",
    "    # Identify indices in FAF field that correspond to the entire dataset (not a subset like non-cancer)\n",
    "    # faf_index_map = [(k,v) for k, v in hl.eval(ds.globals.faf_index_dict).items() if k.startswith(\"gnomad_\")]\n",
    "    from enum import Enum\n",
    "    class FafFilterType(Enum):\n",
    "        # Each correponds to a filter func for a (k,v) of faf label to value\n",
    "        GNOMAD_GLOBAL = lambda t: t[0] == \"gnomad\"\n",
    "        GNOMAD_SUPERPOP = lambda t: t[0].startswith(\"gnomad_\")\n",
    "        ANY = lambda t: True\n",
    "\n",
    "    # By default, filter to superpopulations aggregate faf\n",
    "    default_faf_filter_type = FafFilterType.GNOMAD_SUPERPOP\n",
    "\n",
    "    def faf_filter(faf_idx_tuple:tuple):\n",
    "        return default_faf_filter_type(faf_idx_tuple)\n",
    "\n",
    "    # Get list of the global faf_index_dict which meets the default_faf_filter criteria\n",
    "    # This gives the indices of the desired populations, by default will take all top level populations\n",
    "    faf_index_map = list(filter(faf_filter, [(k,v) for k,v in hl.eval(ds.globals.faf_index_dict).items()]))\n",
    "    faf_indices = [v for k,v in faf_index_map]\n",
    "    faf_labels = [k for k,v in faf_index_map]\n",
    "    \n",
    "    # Annotate table with popmax FAF\n",
    "    \n",
    "    # This only will return the maximum pop FAF for each\n",
    "    # variant, even if multiple populations meet the criteria. \n",
    "    # If we want all matching populations, need an explode() call\n",
    "    # to flatten the pop FAFs into a record per pop per variant\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "        popmax_faf=hl.sorted(\n",
    "            # Take only the FAF entries that correspond to the desired populations (faf_indices)\n",
    "            hl.literal(faf_indices).map(lambda i: ds.faf[i]),\n",
    "            # Sort by 95% confidence FAF\n",
    "            lambda faf_entry: faf_entry.faf95,\n",
    "            # Sort high to low\n",
    "            reverse=True\n",
    "        )[0] # Take the first entry with the highest FAF\n",
    "        ,\n",
    "        # Label of the freq_index_dict entry for this record's max pop\n",
    "        popmax_index_dict_key=hl.sorted(\n",
    "            # List of tuples of (poplabel, faf_index)\n",
    "            list(zip(list(faf_labels), list(faf_indices))),\n",
    "\n",
    "            # Take only the FAF entries that correspond to the entire dataset\n",
    "            # Sort by 95% confidence FAF\n",
    "            key=lambda tpl: ds.faf[tpl[1]].faf95,\n",
    "            # Sort high to low\n",
    "            reverse=True\n",
    "        )[0][0] # Take the first entry, which has the highest FAF\n",
    "    )\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "#         popmax_faf_pop_freq=ds.freq[ds.globals.freq_index_dict[\"gnomad_\" + ds.popmax_faf.meta.get(\"pop\")]]\n",
    "\n",
    "        # ds.globals.freq_index_dict uses the same keys as ds.globals.faf_index_dict so\n",
    "        # we can reuse ds.popmax_index_dict_key created above\n",
    "        popmax_faf_pop_freq=ds.freq[ds.globals.freq_index_dict[ds.popmax_index_dict_key]] \n",
    "    )\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "ds = add_popmax_af(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These next 2 functions override functions from hail.experimental, modified to return a mapping\n",
    "# of gene_symbols to the intervals they correspond to. Existing methods return unordered list\n",
    "\n",
    "import operator\n",
    "import functools\n",
    "from hail.genetics.reference_genome import reference_genome_type\n",
    "from hail.typecheck import typecheck, nullable, sequenceof\n",
    "from hail.utils.java import info\n",
    "from hail.utils import new_temp_file\n",
    "\n",
    "def _load_gencode_gtf(gtf_file=None, reference_genome=None):\n",
    "    \"\"\"\n",
    "    Get Gencode GTF (from file or reference genome)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_genome : :obj:`str` or :class:`.ReferenceGenome`, optional\n",
    "       Reference genome to use (passed along to import_gtf).\n",
    "    gtf_file : :obj:`str`\n",
    "       GTF file to load. If none is provided, but `reference_genome` is one of\n",
    "       `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :class:`.Table`\n",
    "    \"\"\"\n",
    "    GTFS = {\n",
    "        'GRCh37': 'gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz',\n",
    "        'GRCh38': 'gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz',\n",
    "    }\n",
    "    if reference_genome is None:\n",
    "        reference_genome = hl.default_reference().name\n",
    "    else:\n",
    "        reference_genome = reference_genome.name\n",
    "    if gtf_file is None:\n",
    "        gtf_file = GTFS.get(reference_genome)\n",
    "        if gtf_file is None:\n",
    "            raise ValueError(\n",
    "                'get_gene_intervals requires a GTF file, or the reference genome be one of GRCh37 or GRCh38 (when on Google Cloud Platform)')\n",
    "    ht = hl.experimental.import_gtf(gtf_file, reference_genome=reference_genome,\n",
    "                                    skip_invalid_contigs=True, min_partitions=12)\n",
    "    ht = ht.annotate(gene_id=ht.gene_id.split('\\\\.')[0],\n",
    "                     transcript_id=ht.transcript_id.split('\\\\.')[0])\n",
    "    return ht\n",
    "\n",
    "@typecheck(gene_symbols=nullable(sequenceof(str)),\n",
    "           gene_ids=nullable(sequenceof(str)),\n",
    "           transcript_ids=nullable(sequenceof(str)),\n",
    "           verbose=bool, reference_genome=nullable(reference_genome_type), gtf_file=nullable(str))\n",
    "def get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None,\n",
    "                       verbose=True, reference_genome=None, gtf_file=None):\n",
    "    \"\"\"Get intervals of genes or transcripts.\n",
    "\n",
    "    Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.\n",
    "\n",
    "    On Google Cloud platform:\n",
    "    Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz\n",
    "    Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37'))  # doctest: +SKIP\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    gene_symbols : :obj:`list` of :obj:`str`, optional\n",
    "       Gene symbols (e.g. PCSK9).\n",
    "    gene_ids : :obj:`list` of :obj:`str`, optional\n",
    "       Gene IDs (e.g. ENSG00000223972).\n",
    "    transcript_ids : :obj:`list` of :obj:`str`, optional\n",
    "       Transcript IDs (e.g. ENSG00000223972).\n",
    "    verbose : :obj:`bool`\n",
    "       If ``True``, print which genes and transcripts were matched in the GTF file.\n",
    "    reference_genome : :obj:`str` or :class:`.ReferenceGenome`, optional\n",
    "       Reference genome to use (passed along to import_gtf).\n",
    "    gtf_file : :obj:`str`\n",
    "       GTF file to load. If none is provided, but `reference_genome` is one of\n",
    "       `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :obj:`list` of :class:`.Interval`\n",
    "    \"\"\"\n",
    "    if gene_symbols is None and gene_ids is None and transcript_ids is None:\n",
    "        raise ValueError('get_gene_intervals requires at least one of gene_symbols, gene_ids, or transcript_ids')\n",
    "    ht = _load_gencode_gtf(gtf_file, reference_genome)\n",
    "    criteria = []\n",
    "    if gene_symbols:\n",
    "        criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_name == y), gene_symbols))\n",
    "    if gene_ids:\n",
    "        criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\\\.')[0]), gene_ids))\n",
    "    if transcript_ids:\n",
    "        criteria.append(hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\\\.')[0]), transcript_ids))\n",
    "\n",
    "    ht = ht.filter(functools.reduce(operator.ior, criteria))\n",
    "    gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interval)))\n",
    "    if verbose:\n",
    "        info(f'get_gene_intervals found {len(gene_info)} entries:\\n'\n",
    "             + \"\\n\".join(map(lambda x: f'{x[0]}: {x[1]} ({x[2] if x[0] == \"gene\" else x[3]})', gene_info)))\n",
    "    # intervals = list(map(lambda x: x[-1], gene_info))\n",
    "    intervals = list(map(lambda x: {\n",
    "        'gene_symbol': x[1],\n",
    "        'gene_id': x[2],\n",
    "        'transcript_id': x[3],\n",
    "        'interval': x[4]\n",
    "    }, gene_info))\n",
    "    return intervals\n",
    "\n",
    "                         \n",
    "# Look up intervals for the gene symbols in the input thresholds\n",
    "gene_symbols = [k for k in gene_thresholds.keys()]\n",
    "intervals = get_gene_intervals(gene_symbols=gene_symbols, reference_genome=\"GRCh37\")\n",
    "\n",
    "def get_gene_interval(gene_symbol:str):\n",
    "    global intervals\n",
    "    for i in intervals:\n",
    "        if i[\"gene_symbol\"] == gene_symbol:\n",
    "            return i[\"interval\"]\n",
    "    print(\"Getting new gene interval: %s\" % gene_symbol)\n",
    "    i = get_gene_intervals(gene_symbols=[gene_symbol], reference_genome=\"GRCh37\")[0]\n",
    "    intervals.append(i)\n",
    "    return i[\"interval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some preliminary annotations\n",
    "ds_crit = ds\n",
    "\n",
    "# This was removed because we can't assume all gene symbols are the same, a variant can have >1\n",
    "# ds_crit = ds_http://localhost:8123/notebooks/clingen-dataproc-workspace-kferrite/ClinGen-Gnomad-FAF-Report-V2.ipynb#crit.annotate(\n",
    "#     gene_symbol=ds_crit.vep.transcript_consequences.gene_symbol # Can't assume they are all the same\n",
    "# )\n",
    "\n",
    "print(intervals)\n",
    "ivl_struct_list = hl.literal(\n",
    "    [hl.struct(\n",
    "        gene_symbol=i[\"gene_symbol\"],\n",
    "        gene_id=i[\"gene_id\"],\n",
    "        transcript_id=i[\"transcript_id\"],\n",
    "        interval=i[\"interval\"]\n",
    "    ) for i in intervals]\n",
    ")\n",
    "\n",
    "# Filter by intervals of genes provided in input criteria\n",
    "ds_crit = hl.filter_intervals(ds_crit, [i[\"interval\"] for i in intervals])\n",
    "\n",
    "# Now attach the gene field using 1 of two methods.\n",
    "# If transcript_filter is true, attach gene label based on transcript_consequences\n",
    "# If transcript_filter is false, attach based on which gene interval it is contained in\n",
    "if transcript_filter is False:\n",
    "    ds_crit = ds_crit.annotate(\n",
    "        gene=ivl_struct_list.find(\n",
    "            # Check if Interval object contains this variant (start pos)\n",
    "            lambda ivl: ivl[\"interval\"].contains(ds_crit.locus)\n",
    "        ).gene_symbol\n",
    "    )\n",
    "else:\n",
    "    # Explode a new record per transcript consequence, each now has 1 gene\n",
    "    ds_crit = ds_crit.annotate(\n",
    "        transcript_consequences=ds_crit.vep.transcript_consequences\n",
    "    )\n",
    "    ds_crit = ds_crit.explode(\"transcript_consequences\")\n",
    "    ds_crit = ds_crit.annotate(\n",
    "        gene=ds_crit.transcript_consequences.gene_symbol\n",
    "    )\n",
    "\n",
    "\n",
    "# Sort each gene's criteria thresholds descending by AF so first hl.find is the max\n",
    "gene_thresholds_sorted = gene_thresholds_exp.map_values(\n",
    "    lambda gene_criteria: hl.sorted(\n",
    "        # Transform {\"BA1\": {\"AF\": 0.02}} to list of [(\"BA1\", {\"AF\": 0.02})]\n",
    "        gene_criteria.keys().map(lambda crit_name: (crit_name, gene_criteria[crit_name])),\n",
    "        \n",
    "        # Key to sort the above ArrayExpression\n",
    "        lambda t: t[1][\"AF\"],\n",
    "        \n",
    "        # Reverse order so we find the max threshold first\n",
    "        reverse=True\n",
    "    )\n",
    ")\n",
    "print(gene_thresholds_sorted.collect())\n",
    "\n",
    "# Filter to variants in genes we care about\n",
    "ds_crit = ds_crit.filter(\n",
    "    gene_thresholds_exp.keys().contains(ds_crit.gene)\n",
    ")\n",
    "\n",
    "ds_crit = ds_crit.annotate(\n",
    "    # Get the max AF threshold which is less or equal to popmax_faf.faf95\n",
    "    criteria_satisfied=hl.or_missing(\n",
    "        # Condition\n",
    "        gene_thresholds_exp.keys().any(\n",
    "            lambda threshold_gene: ds_crit.gene == threshold_gene\n",
    "        ),\n",
    "        \n",
    "        # If this gene is in criteria, find max criteria (already reverse sorted, find gets first)\n",
    "        hl.find(\n",
    "            lambda tpl: tpl[1][\"AF\"] <= ds_crit.popmax_faf.faf95,\n",
    "            # gene_thresholds[ds_crit.gene][crit_name][\"AF\"] <= ds_crit.popmax_faf.faf95,\n",
    "\n",
    "            # List of (crit_name, {\"AF\": 0.02})\n",
    "            gene_thresholds_sorted[ds_crit.gene]\n",
    "        )[0] # returns the criteria name (ex: BA1)\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter to variants which meet a criteria\n",
    "ds_crit = ds_crit.filter(\n",
    "    ~hl.is_missing(ds_crit.criteria_satisfied)\n",
    ")\n",
    "\n",
    "\n",
    "filtered_ds = ds_crit.select(\n",
    "    criteria_satisfied = ds_crit.criteria_satisfied,\n",
    "    source = ds_crit.source,\n",
    "    gene = ds_crit.gene,\n",
    "    popmax_pop = ds_crit.popmax_faf.meta[\"pop\"],\n",
    "    popmax_ac = ds_crit.popmax_faf_pop_freq.AC,\n",
    "    popmax_an = ds_crit.popmax_faf_pop_freq.AN,\n",
    "    faf95 = ds_crit.popmax_faf.faf95,\n",
    "    genomic_coordinates = hl.format(\"%s-%s-%s-%s\",\n",
    "        ds_crit.locus.contig,\n",
    "        hl.str(ds_crit.locus.position),\n",
    "        ds_crit.alleles[0],\n",
    "        ds_crit.alleles[1]\n",
    "    )\n",
    ")\n",
    "# filtered_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import ClinVar VCF as Hail Table\n",
    "# clinvar = hl.import_vcf(\"/path/to/clinvar.vcf.gz\", force_bgz=True, drop_samples=True, skip_invalid_loci=True).rows()\n",
    "\n",
    "# Download clinvar BGZF\n",
    "import os, requests, subprocess\n",
    "\n",
    "# Function to download a file to a localpath. ClinVar VCF is small enough to download to dataproc default local disk.\n",
    "def download_to_file(url, filepath):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(filepath, \"wb\") as fout: \n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "             if chunk:\n",
    "                 fout.write(chunk)\n",
    "# This url always points to the latest dump file, updated periodically by ClinVar\n",
    "clinvar_vcf_url = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/clinvar.vcf.gz\"\n",
    "clinvar_vcf_localpath = \"/home/hail/clinvar.vcf.gz\"\n",
    "clinvar_vcf_hdfs = \"clinvar.vcf.gz\"\n",
    "download_to_file(clinvar_vcf_url, clinvar_vcf_localpath)\n",
    "assert(os.path.exists(clinvar_vcf_localpath))\n",
    "print(\"Downloaded ClinVar VCF, file size (expecting ~28M): %d\" % os.path.getsize(clinvar_vcf_localpath))\n",
    "\n",
    "# Hail needs the file in HDFS\n",
    "p = subprocess.Popen([\"hdfs\", \"dfs\", \"-cp\", \"file://\" + clinvar_vcf_localpath, clinvar_vcf_hdfs])\n",
    "print(p.communicate())\n",
    "\n",
    "\n",
    "clinvar = hl.import_vcf(\n",
    "    clinvar_vcf_hdfs,\n",
    "    force_bgz=True,\n",
    "    drop_samples=True, \n",
    "    skip_invalid_loci=True\n",
    ").rows()\n",
    "print(clinvar.count())\n",
    "\n",
    "# Join ClinVar table to gnomAD table. ClinVar fields available under the table.clinvar struct\n",
    "gnomad_clinvar_ds = filtered_ds.annotate(\n",
    "    clinvar=clinvar[filtered_ds.locus, filtered_ds.alleles]\n",
    ")\n",
    "\n",
    "# ClinVar VCF export sets ID column to the ClinVar Variation ID (not rsid)\n",
    "# And sets the RS field of INFO to the rsid if it exists.\n",
    "# (https://ftp.ncbi.nlm.nih.gov/pub/clinvar/README_VCF.txt)\n",
    "# Hail then sets this ClinVar ID as the rsid column of the clinvar struct\n",
    "# We can filter to only the variants that exist in clinvar with:\n",
    "# gnomad_clinvar_ds = gnomad_clinvar_ds.filter(\n",
    "#     ~hl.is_missing(gnomad_clinvar_ds.clinvar_rsid)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading ClinGen Allele Registry gene info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map genes to MANE Preferred ref seqs\n",
    "# example: http://reg.genome.network/genes?name=MYH7\n",
    "import json\n",
    "\n",
    "gene_url_templ = \"http://reg.genome.network/genes?name={gene_name}\"\n",
    "\n",
    "genes_MANE_pref_map = {}\n",
    "\n",
    "def list_flatten(L):\n",
    "    return [e for l in L for e in l]\n",
    "\n",
    "# input_genes = list_flatten([t.keys() for t in gene_thresholds.collect()])\n",
    "input_genes = list(gene_thresholds.keys())\n",
    "\n",
    "for gene_name in input_genes:\n",
    "    gene_url = gene_url_templ.format(gene_name=gene_name)\n",
    "    response = requests.get(gene_url)\n",
    "    if response.status_code != 200:\n",
    "        handle_http_error(response)\n",
    "    r = json.loads(response.text)\n",
    "    # Filter to only those which have this gene symbol (discard overlappings)\n",
    "    r = [e for e in r if e[\"externalRecords\"][\"HGNC\"][\"symbol\"] == gene_name]\n",
    "    \n",
    "    if len(r) == 0:\n",
    "        raise RuntimeError(\"Gene name %s not found in allele registry /genes\" % gene_name)\n",
    "    elif len(r) > 1:\n",
    "        print(\"Warning: gene name %s returned multiple entries from allele registry /genes: %s\" % (\n",
    "            gene_name, json.dumps(r, indent=2)))\n",
    "    g = r[0] # Select first gene sequence object\n",
    "    external_records = g[\"externalRecords\"]\n",
    "    if external_records[\"HGNC\"][\"symbol\"] != gene_name:\n",
    "        raise RuntimeError(\"Unexpected symbol %s returned from %s query\" % (\n",
    "            external_records[\"HGNC\"][\"symbol\"], gene_name))\n",
    "    mane_pref_refseq = external_records[\"MANEPrefRefSeq\"][\"id\"]\n",
    "    \n",
    "    # Add to mapping\n",
    "    genes_MANE_pref_map[gene_name] = mane_pref_refseq\n",
    "\n",
    "\n",
    "# For genes not in gene_refseqs (from user input), set refseq to MANE Preferred\n",
    "for gene_name in input_genes:\n",
    "    if gene_name not in gene_refseqs:\n",
    "        print(\"Updating gene %s refseq to MANE Preferred %s\" % (\n",
    "            gene_name, genes_MANE_pref_map[gene_name]))\n",
    "        gene_refseqs[gene_name] = genes_MANE_pref_map[gene_name]\n",
    "\n",
    "# This is just for debugging/checking later, can be removed if later block referencing it is removed\n",
    "\n",
    "print(\"Processing genes_MANE_pref_map names and versions\")\n",
    "genes_MANE_pref_map_split = {}\n",
    "for gene_name, refseq in genes_MANE_pref_map.items():\n",
    "    r_id, r_version = refseq.split(\".\")\n",
    "    genes_MANE_pref_map_split[gene_name] = {\n",
    "        \"refseq\": refseq,\n",
    "        \"refseq_id\": r_id,\n",
    "        \"refseq_version\": r_version\n",
    "    }\n",
    "genes_MANE_pref_map_split_hl = hl.literal(genes_MANE_pref_map_split)\n",
    "\n",
    "print(json.dumps(gene_refseqs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading ClinGen Allele Registry variant info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add information from the ClinGen Allele Registry\n",
    "import requests\n",
    "import json\n",
    "allele_registry_baseurl = \"https://reg.clinicalgenome.org\"\n",
    "\n",
    "# If the collected rows are the gnomad_ids can't fit into memory, would need to write out\n",
    "# to a file and connect the request to the file stream. As is, is generally a few 100KiB or few MiB\n",
    "# def write_col_to_file(ds, colname:str, filename:str):\n",
    "#     ds.select(colname).export(filename, header=False)\n",
    "# gnomad_id_file = \"gnomad-ids.txt\"\n",
    "# write_col_to_file(gnomad_clinvar_ds, \"genomic_coordinates\", gnomad_id_file)\n",
    "\n",
    "allele_registry_ds = gnomad_clinvar_ds\n",
    "\n",
    "def handle_http_error(response):\n",
    "    raise RuntimeError(\"Request failed (status code %s), response: %s\" % (response.status_code, response.text))\n",
    "\n",
    "gnomad_ids = allele_registry_ds.select(\"genomic_coordinates\").collect()\n",
    "# The above returns structs which still contain locus and position keys\n",
    "gnomad_ids = [e.genomic_coordinates for e in gnomad_ids]\n",
    "post_body = \"\\n\".join(gnomad_ids)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    allele_registry_baseurl + \"/alleles?file=gnomAD.id\",\n",
    "    data=post_body\n",
    ")\n",
    "if response.status_code != 200:\n",
    "    handle_http_error(response)\n",
    "\n",
    "print(\"Downloaded %d bytes from allele registry\" % len(response.text))\n",
    "reg_entries_d = json.loads(response.text) # list\n",
    "\n",
    "# Remove response from memory after parsing\n",
    "del response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register missing variants in the ClinGen Allele Registry\n",
    "import hashlib, time\n",
    "\n",
    "# def register_alleles(identifiers:list, identifier_type:str):\n",
    "#     url = allele_registry_baseurl\n",
    "#     url = \"https://reg.clinicalgenome.org\"\n",
    "#     login = \"PROVIDE_CREDENTIALS\"\n",
    "#     password = \"PROVIDE_CREDENTIALS\"\n",
    "#     identity = hashlib.sha1((login + password).encode('utf-8')).hexdigest()\n",
    "    \n",
    "#     gbTime = str(int(time.time()))\n",
    "    \n",
    "#     token = hashlib.sha1((url + identity + gbTime).encode('utf-8')).hexdigest()\n",
    "#     url_params = { # values must be url safe\n",
    "#         \"file\": \"gnomAD.id\",\n",
    "#         \"gbLogin\": login,\n",
    "#         \"gbTime\": gbTime,\n",
    "#         \"gbToken\": token\n",
    "#     }\n",
    "#     url = url + \"/alleles?\" + \"&\".join(p[0]+\"=\"+p[1] for p in url_params.items())\n",
    "#     print(url)\n",
    "#     response = requests.put(\n",
    "#         url,\n",
    "#         data=\"\\n\".join(identifiers))\n",
    "#     return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate the response from the Allele Registry to filter it down and map it to keys\n",
    "# that we can use later in annotations\n",
    "\n",
    "# Store the alleles which the ClinGen Allele Registry does not have registered\n",
    "unregistered_gnomad_ids = []\n",
    "\n",
    "# Map gnomad IDs to allele registry entries\n",
    "gnomad_reg_map = {}\n",
    "\n",
    "for reg_entry in reg_entries_d:\n",
    "    if \"genomicAlleles\" not in reg_entry:\n",
    "        if reg_entry.get(\"errorType\", \"\"):\n",
    "            # print(\"Unregistered gnomAD id: \" + reg_entry[\"inputLine\"])\n",
    "            unregistered_gnomad_ids.append(reg_entry[\"inputLine\"])\n",
    "#             gnomad_reg_map[reg_entry[\"inputLine\"]] = {}\n",
    "            continue\n",
    "        else:\n",
    "            raise RuntimeError(\"genomicAlleles field not in response entry from allele registry:\\n%s\" % reg_entry)\n",
    "    desired_genomic_allele = None\n",
    "    for genomic_allele in reg_entry[\"genomicAlleles\"]:\n",
    "        if \"referenceGenome\" not in genomic_allele:\n",
    "            raise RuntimeError(\"referenceGenome is not in genomicAllele entry\")\n",
    "        if genomic_allele[\"referenceGenome\"] == reference_genome:\n",
    "            desired_genomic_allele = genomic_allele\n",
    "            gnomad_id = str(reg_entry[\"externalRecords\"][\"gnomAD\"][0][\"id\"])\n",
    "            gnomad_reg_map[gnomad_id] = reg_entry\n",
    "            break\n",
    "    if desired_genomic_allele is None:\n",
    "        raise RuntimeError(\"Did not find genomic allele with reference %s\" % reference_genome)\n",
    "\n",
    "if len(unregistered_gnomad_ids) > 0:\n",
    "    print(\"gnomAD ids not registered in Allele Registry:\")\n",
    "    for u in unregistered_gnomad_ids:\n",
    "        print(u)\n",
    "\n",
    "\n",
    "# Pre-process the accession identifier and version\n",
    "print(\"Processing gene_refseqs names and versions\")\n",
    "gene_refseqs_split = {}\n",
    "for gene_name, refseq in gene_refseqs.items():\n",
    "    r_id, r_version = refseq.split(\".\")\n",
    "    gene_refseqs_split[gene_name] = {\n",
    "        \"refseq\": refseq,\n",
    "        \"refseq_id\": r_id,\n",
    "        \"refseq_version\": r_version\n",
    "    }\n",
    "\n",
    "\n",
    "# Use gnomad ID -> allele registry entry important info\n",
    "gnomad_hgvs_map = {}\n",
    "\n",
    "for gnomad_id, reg_entry in gnomad_reg_map.items():\n",
    "    # Get protein change\n",
    "    if \"transcriptAlleles\" in reg_entry:\n",
    "        transcript_alleles = reg_entry[\"transcriptAlleles\"]\n",
    "        gnomad_id_entry_count = len(gnomad_hgvs_map.get(gnomad_id, []))\n",
    "        for transcript_allele in transcript_alleles:\n",
    "            transcript_gene = transcript_allele.get(\"geneSymbol\", None)\n",
    "#             protein_effect = hl.null(\"str\")\n",
    "            protein_effect = \"\" # Empty string default for compatibility with string operations\n",
    "            \n",
    "            if transcript_gene in input_genes:\n",
    "                if \"proteinEffect\" in transcript_allele:\n",
    "                    protein_effect = transcript_allele[\"proteinEffect\"][\"hgvs\"]\n",
    "                \n",
    "                for hgvs in transcript_allele[\"hgvs\"]:\n",
    "                    # Take the entry if it has an hgvs expression with the same refseq_id\n",
    "                    # even if the version is different. We collapse to 1 preferred in a later step\n",
    "                    if hgvs.startswith(gene_refseqs_split[transcript_gene][\"refseq_id\"]):\n",
    "                        if gnomad_id not in gnomad_hgvs_map:\n",
    "                            gnomad_hgvs_map[gnomad_id] = []\n",
    "                        \n",
    "                        h_terms = hgvs.replace(\":\", \".\").split(\".\")\n",
    "                        h_id, h_version = h_terms[0], h_terms[1]\n",
    "#                         p_terms = protein_effect.replace(\":\", \".\").split(\".\")\n",
    "#                         p_id, p_version = p_terms[0], p_terms[1]\n",
    "#                         print(\"hgvs: %s, seq: %s, version: %s\" % (hgvs, h_id, h_version))\n",
    "#                         print(\"protein_effect: %s, seq: %s, version: %s\" % (protein_effect, p_id, p_version))\n",
    "                        \n",
    "                        gnomad_hgvs_map[gnomad_id].append({\n",
    "                            \"refseq\": hgvs,\n",
    "                            \"refseq_id\": h_id,\n",
    "                            \"refseq_version\": h_version,\n",
    "                            \"protein_effect\": protein_effect\n",
    "                        })\n",
    "        print(\"Added %s entries for gnomad_id %s\" % (len(gnomad_hgvs_map.get(gnomad_id, [])), gnomad_id))\n",
    "    else:\n",
    "        print(\"transcriptAlleles not in reg_entry:\\n%s\" % json.dumps(reg_entry))\n",
    "\n",
    "print(\"Freeing gnomad_reg_map\")\n",
    "del gnomad_reg_map\n",
    "\n",
    "print(\"gnomad_hgvs_map len: %d\" % (len(gnomad_hgvs_map)))\n",
    "print(\"was expecting: %d\" % len(gnomad_ids))\n",
    "# gnomad_hgvs_hl = hl.literal(gnomad_hgvs_map)\n",
    "\n",
    "# gene_refseqs_hl = hl.literal(gene_refseqs)\n",
    "print(\"Converting objects to hail expressions\")\n",
    "gene_refseqs_split_hl = hl.literal(gene_refseqs_split)\n",
    "input_genes_hl = hl.literal(input_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gnomad_hgvs_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Adding annotations\")\n",
    "\n",
    "reg_ds = gnomad_clinvar_ds\n",
    "\n",
    "import pandas as pd\n",
    "# from pyspark.sql import Row, SparkSession\n",
    "import numpy as np\n",
    "print(\"Converting gnomad_hgvs_map to Hail Table\")\n",
    "\n",
    "print(type(gnomad_hgvs_map))\n",
    "\n",
    "def convert_gnomad_hgvs_map_to_hail_table1():\n",
    "    \"\"\"\n",
    "    Had some issue with type conversion here but may be resolved. It may be better to use this\n",
    "    instead of going through HDFS directly as in the other version of this function\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for gnomad_id, hgvs_list in gnomad_hgvs_map.items():\n",
    "        for hgvs_entry in hgvs_list:\n",
    "            \n",
    "            row = [gnomad_id,\n",
    "                   hgvs_entry[\"refseq\"],\n",
    "                   hgvs_entry[\"refseq_id\"],\n",
    "                   hgvs_entry[\"refseq_version\"],\n",
    "                   hgvs_entry[\"protein_effect\"]]\n",
    "            rows.append(row)\n",
    "            print(row)\n",
    "    df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"gnomad_id\", \n",
    "            \"refseq\", \n",
    "            \"refseq_id\",\n",
    "            \"refseq_version\",\n",
    "            \"protein_effect\"]\n",
    "    )\n",
    "    print(df)\n",
    "    print(df.dtypes)\n",
    "    print(df.describe())\n",
    "    ht = hl.Table.from_pandas(df, key=[\"gnomad_id\"])\n",
    "    return ht\n",
    "\n",
    "def convert_gnomad_hgvs_map_to_hail_table():\n",
    "    \"\"\"\n",
    "    This dumps the stripped-down allele registry response with gnomad-ids to\n",
    "    HDFS and imports it as a Hail Table for doing a join to the overall table\n",
    "    \n",
    "    Converting it to an in-memory DictExpression for the annotations results\n",
    "    in higher memory usage which can crash the Java Spark backend\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    temp_file = \"gnomad-hgvs.tsv\"\n",
    "    os.system(\"rm %s\" % temp_file)\n",
    "    with open(temp_file, \"w\") as f_out:\n",
    "        # Write headers\n",
    "        headers = [\n",
    "            \"gnomad_id\", \"refseq\", \"refseq_id\", \"refseq_version\", \"protein_effect\"]\n",
    "        f_out.write(\"\\t\".join(headers))\n",
    "        f_out.write(\"\\n\")\n",
    "        for gnomad_id, hgvs_list in gnomad_hgvs_map.items():\n",
    "            for hgvs_entry in hgvs_list:\n",
    "                row = [gnomad_id,\n",
    "                       hgvs_entry[\"refseq\"],\n",
    "                       hgvs_entry[\"refseq_id\"],\n",
    "                       hgvs_entry[\"refseq_version\"],\n",
    "                       hgvs_entry[\"protein_effect\"]]\n",
    "#                 print(row)\n",
    "                f_out.write(\"\\t\".join(row))\n",
    "                f_out.write(\"\\n\")\n",
    "    # Copy to HDFS\n",
    "    temp_file_path = os.path.join(os.getcwd(), temp_file)\n",
    "    p = subprocess.Popen([\"hdfs\", \"dfs\", \"-rm\", \"-f\", temp_file],\n",
    "            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "    if len(stderr) > 0:\n",
    "        raise RuntimeError(\"stdout:\\n%s\\nstderr:\\n%s\\n\" % (stdout, stderr))\n",
    "    p = subprocess.Popen([\"hdfs\", \"dfs\", \"-cp\", \"file://\"+temp_file_path, temp_file],\n",
    "            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "    if len(stderr) > 0:\n",
    "        raise RuntimeError(\"stdout:\\n%s\\nstderr:\\n%s\\n\" % (stdout, stderr))\n",
    "    ht = hl.import_table(temp_file)\n",
    "    ht = ht.key_by(ht.gnomad_id)\n",
    "    return ht\n",
    "\n",
    "ht = convert_gnomad_hgvs_map_to_hail_table()\n",
    "\n",
    "refseqs_array_hl = gene_refseqs_split_hl.values()\n",
    "\n",
    "# ht.filter(ht.gnomad_id==\"1-115248097-T-C\").show()\n",
    "\n",
    "# Only take the preferred ref sequence or if preferred ref version isn't present,\n",
    "# take the highest version for the same sequence identifier\n",
    "ht = ht.filter(\n",
    "    (\n",
    "        # If exact match, keep\n",
    "        refseqs_array_hl.any(\n",
    "            lambda r: r[\"refseq\"] == ht.refseq\n",
    "        ) |\n",
    "        # If not exact match and is the highest for this sequence identifier\n",
    "        (\n",
    "            ~refseqs_array_hl.any(lambda r: r[\"refseq\"] == ht.refseq)) \\\n",
    "             & \\\n",
    "            (ht.refseq_version == hl.str(hl.max(refseqs_array_hl.filter(\n",
    "                lambda r: r[\"refseq_id\"] == ht.refseq_id\n",
    "            ).map(\n",
    "                lambda r: hl.int(r[\"refseq_version\"])\n",
    "            )))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# ht.filter(ht.gnomad_id==\"1-115248097-T-C\").show()\n",
    "\n",
    "def get_hgvs_by_coordinates(genomic_coordinates):\n",
    "    ret = gnomad_hgvs_map.get(genomic_coordinates)\n",
    "    if ret is None:\n",
    "        raise RuntimeError(\"None on \" + genomic_coordinates)\n",
    "    return ret\n",
    "\n",
    "# This join works as long as there is a 1:1 mapping along the key reg_ds.genomic_coordinates <-> ht.gnomad_id\n",
    "# Otherwise it may just select one of the matches arbitrarily. To do a proper join in that case, use Table.join\n",
    "reg_ds = reg_ds.annotate(\n",
    "    hgvs=ht[reg_ds.genomic_coordinates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some quick sanity checking. These do not affect the report file.\n",
    "\n",
    "# reg_ds.describe()\n",
    "\n",
    "\n",
    "# # Records missing protein_effect\n",
    "# no_protein_effect_ht = reg_ds.filter(\n",
    "#     hl.is_missing(reg_ds.hgvs.protein_effect) | (reg_ds.hgvs.protein_effect.length() == 0))\n",
    "# print(\"%s records were missing protein_effect\" % (no_protein_effect_ht.count()))\n",
    "# no_protein_effect_ht.show()\n",
    "\n",
    "\n",
    "# Records missing hgvs refseq \n",
    "# no_hgvs_refseq_ht = reg_ds.filter(\n",
    "#     hl.is_missing(reg_ds.hgvs.refseq) | (reg_ds.hgvs.refseq.length() == 0))\n",
    "# print(\"%s out of %s records were missing refseq\" % (no_hgvs_refseq_ht.count(), reg_ds.count()))\n",
    "# no_hgvs_refseq_ht.show()\n",
    "\n",
    "\n",
    "# Records with refseq included for different version from MANE preferred\n",
    "# refseq_version_mismatch = reg_ds.filter(\n",
    "#     (genes_MANE_pref_map_split_hl[reg_ds.gene][\"refseq_id\"] == reg_ds.hgvs.refseq_id) \\\n",
    "#     & (genes_MANE_pref_map_split_hl[reg_ds.gene][\"refseq_version\"] != reg_ds.hgvs.refseq_version)\n",
    "# )\n",
    "# print(\"%s records had refseq preferred version mismatch\" % (refseq_version_mismatch.count()))\n",
    "# refseq_version_mismatch.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old annotations\n",
    "# gnomad_clinvar_allele_reg_ds = gnomad_clinvar_ds.annotate(\n",
    "#     hgvs=hl.or_missing(\n",
    "#         input_genes_hl.contains(gnomad_clinvar_ds.gene),\n",
    "        \n",
    "#         # If the record is in one of the input genes, do the following\n",
    "#         hl.or_else(\n",
    "#             # First check if there is an exact match for the preferred gene refseq version\n",
    "#             gnomad_hgvs_hl.get(gnomad_clinvar_ds.genomic_coordinates).find(\n",
    "#                 lambda entry: entry[\"refseq\"] == gene_refseqs_split_hl.get(gnomad_clinvar_ds.gene)[\"refseq\"]\n",
    "#             ),\n",
    "            \n",
    "#             # If not found, use the highest refseq with same identifier ignoring version\n",
    "#             # if none, this returns missing\n",
    "#             hl.sorted(\n",
    "#                 # Collection of possible entries (filtered to those with same unversioned seq identifier)\n",
    "#                 gnomad_hgvs_hl.values().filter(\n",
    "#                     lambda e: e[\"refseq_id\"] == gene_refseqs_split_hl.get(gnomad_clinvar_ds.gene)[\"refseq_id\"]\n",
    "#                 ),\n",
    "#                 # Key to sort on (version integer)\n",
    "#                 key=lambda e: int(e[\"refseq_version\"]),\n",
    "#                 # Reverse for descending order\n",
    "#                 reverse=True\n",
    "#             )[0] # take highest\n",
    "#         ).get(\"refseq\") # Use whole accession.version value\n",
    "#     ),\n",
    "#     # Same as above hgvs field but other value in the gnomad_hgvs_hl elemenet\n",
    "#     protein_effect=hl.or_missing(\n",
    "#         input_genes_hl.contains(gnomad_clinvar_ds.gene),\n",
    "        \n",
    "#         # If the record is in one of the input genes, do the following\n",
    "#         hl.or_else(\n",
    "#             # First check if there is an exact match for the preferred gene refseq version\n",
    "#             gnomad_hgvs_hl.get(gnomad_clinvar_ds.genomic_coordinates).find(\n",
    "#                 lambda entry: entry[\"refseq\"] == gene_refseqs_split_hl.get(gnomad_clinvar_ds.gene)[\"refseq\"]\n",
    "#             ),\n",
    "            \n",
    "#             # If not found, use the highest refseq with same identifier ignoring version\n",
    "#             # if none, this returns missing\n",
    "#             hl.sorted(\n",
    "#                 # Collection of possible entries (filtered to those with same unversioned seq identifier)\n",
    "#                 gnomad_hgvs_hl.values().filter(\n",
    "#                     lambda e: e[\"refseq_id\"] == gene_refseqs_split_hl.get(gnomad_clinvar_ds.gene)[\"refseq_id\"]\n",
    "#                 ),\n",
    "#                 # Key to sort on (version integer)\n",
    "#                 key=lambda e: int(e[\"refseq_version\"]),\n",
    "#                 # Reverse for descending order\n",
    "#                 reverse=True\n",
    "#             )[0] # take highest\n",
    "#         ).get(\"protein_effect\")\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished transcript hgvs and protein annotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select desired output fields (columns are ordered as provided)\n",
    "output_ds = reg_ds\n",
    "\n",
    "output_ds = output_ds.select(\n",
    "    output_ds.criteria_satisfied,\n",
    "    output_ds.gene,\n",
    "    output_ds.faf95,\n",
    "    output_ds.source,\n",
    "    output_ds.popmax_pop,\n",
    "    output_ds.popmax_ac,\n",
    "    output_ds.popmax_an,\n",
    "    output_ds.genomic_coordinates,\n",
    "    hgvs=output_ds.hgvs.refseq,\n",
    "    protein_effect=output_ds.hgvs.protein_effect,\n",
    "    clinvar_variation_id=output_ds.clinvar.rsid,\n",
    "    clinvar_review_status=hl.delimit(output_ds.clinvar.info[\"CLNREVSTAT\"], \",\"),\n",
    "    clinvar_significance=hl.delimit(output_ds.clinvar.info[\"CLNSIG\"], \",\"),\n",
    "    clinvar_significance_interpretations=hl.delimit(output_ds.clinvar.info[\"CLNSIGCONF\"], \",\")\n",
    ")\n",
    "\n",
    "# output_ds.describe()\n",
    "\n",
    "# Export to TSV\n",
    "report_filename = \"report.tsv\"\n",
    "import time\n",
    "print(\"Starting export to %s\" % report_filename)\n",
    "start_time = time.time()\n",
    "output_ds.export(report_filename)\n",
    "end_time = time.time()\n",
    "print(\"Export took %.2f seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The export is in HDFS now, copy to machine-local file\n",
    "report_localpath = os.path.join(os.getcwd(), report_filename)\n",
    "os.system(\"rm %s\" % report_localpath)\n",
    "p = subprocess.Popen([\"hdfs\", \"dfs\", \"-cp\", report_filename, \"file://\" + report_localpath],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(p.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to bucket and filepath set at top of notebook\n",
    "gs_output_file = \"gs://%s/%s\" % (output_bucket, output_filename)\n",
    "p = subprocess.Popen([\"gsutil\", \"cp\", report_localpath, gs_output_file],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(p.communicate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
