{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####\n",
    "# This notebook imports the following data sources:\n",
    "#  - gnomAD v2.1 hail table\n",
    "#  - ClinVar GRCh37 VCF\n",
    "# The update to gnomad v3 presented significant changes to the schema of metadata which \n",
    "# breaks the handling of AF and populations in this notebook.\n",
    "# It also renamed the contigs in the ref column which breaks the join with clinvar.\n",
    "#\n",
    "\"\"\"\n",
    "ClinVar:\n",
    "- clinvar_variation_id\n",
    "- clinvar_review_status\n",
    "- clinvar_significance\n",
    "- clinvar_significance_interpretations\n",
    "- locus + alleles\n",
    "GnomAD:\n",
    "- faf95\n",
    "- source (genomes | exomes)\n",
    "- popmax_pop\n",
    "- popmax_ac\n",
    "- popmax_an\n",
    "- genomic_coordinates\n",
    "\"\"\"\n",
    "\n",
    "# Set the output bucket to write to, dataproc service account must have write access\n",
    "# Do not include trailing slash or \"gs://\"\n",
    "output_bucket = \"\"\n",
    "assert(len(output_bucket) > 0)\n",
    "# Set the TSV path to write into bucket. Can contain slash like \"folder/file.tsv\"\n",
    "# Do not include leading slash\n",
    "output_filename = \"report.tsv\"\n",
    "\n",
    "reference_genome = \"GRCh37\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "# `idempontent=True` is useful for running all cells in the notebook\n",
    "hl.init(idempotent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# utility functions for file placement\n",
    "import subprocess\n",
    "import os\n",
    "import time, datetime\n",
    "\n",
    "def run_args(args, fail_on_stderr=False, success_codes=[0]) -> tuple: # (stdout,stderr,returncode)\n",
    "    print(args)\n",
    "    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "    if (fail_on_stderr and len(stderr) > 0) or (p.returncode not in success_codes):\n",
    "        raise RuntimeError(\"command {} failed with code {}:{}\".format(\n",
    "            args, p.returncode, stderr))\n",
    "    return (stdout, stderr, p.returncode)\n",
    "\n",
    "def local_to_bucket(local_path:str, gcs_path:str):\n",
    "    if not gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = \"gs://{}/{}\".format(output_bucket, gcs_path)\n",
    "    args = [\"gsutil\", \"cp\", local_path, gcs_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def bucket_to_local(gcs_path:str, local_path:str):\n",
    "    if not gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = \"gs://{}/{}\".format(output_bucket, gcs_path)\n",
    "    args = [\"gsutil\", \"cp\", gcs_path, local_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def local_to_hdfs(local_path:str, hdfs_path:str):\n",
    "    if not local_path.startswith(\"/\"):\n",
    "        local_path = os.path.join(os.getcwd(), local_path)\n",
    "    args = [\"hdfs\", \"dfs\", \"-rm\", hdfs_path]\n",
    "    run_args(args, success_codes=[0,1]) # Allow error\n",
    "    args = [\"hadoop\", \"fs\", \"-cp\", \"file://\" + local_path, hdfs_path]\n",
    "#     args = [\"hdfs\", \"dfs\", \"-cp\", \"file://\" + local_path, hdfs_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def hdfs_to_local(hdfs_path:str, local_path:str):\n",
    "    if os.path.exists(local_path):\n",
    "        os.remove(local_path)\n",
    "    args = [\"hdfs\", \"dfs\", \"-cp\", hdfs_path, \"file://\" + local_path]\n",
    "    run_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import io, re\n",
    "\n",
    "import os\n",
    "# os.listdir(\"/data/gnomad.genomes.r2.1.1.sites.ht\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "# Read gnomAD data as Hail Tables\n",
    "# ds_exomes = hl.read_table(\"gs://gnomad-public/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\")\n",
    "# ds_exomes = hl.read_table(\n",
    "#     \"gs://gnomad-public-requester-pays/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\")\n",
    "\n",
    "# ds_exomes = ds_exomes.annotate(\n",
    "#     source=\"gnomAD Exomes\"\n",
    "# )\n",
    "# ds_genomes = hl.read_table(\"gs://gnomad-public/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht\")\n",
    "ds_genomes = hl.read_table(\n",
    "    #\"gs://gnomad-public-requester-pays/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht\"\n",
    "    \"gs://gcp-public-data--gnomad/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht\"\n",
    ")\n",
    "ds_genomes = ds_genomes.annotate(\n",
    "    source=\"gnomAD Genomes\"\n",
    ")\n",
    "\n",
    "# Can perform a union here if wanting both (ds = ds1.union(ds2))\n",
    "def select_necessary_cols(ds):\n",
    "    ds = ds.select(ds.freq, ds.faf, ds.vep, ds.source)\n",
    "    return ds\n",
    "\n",
    "# ds_exomes = select_necessary_cols(ds_exomes)\n",
    "ds_genomes = select_necessary_cols(ds_genomes)\n",
    "\n",
    "# Not using VEP in this notebook.\n",
    "ds_genomes = ds_genomes.drop(ds_genomes.vep)\n",
    "gnomad = ds_genomes\n",
    "# ds = ds_genomes.union(ds_exomes, unify=True)\n",
    "\n",
    "# Show the schema of the hail Table\n",
    "gnomad.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(gnomad.n_partitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ds.describe()\n",
    "# testds = ds.filter(ds.locus==hl.locus(\"16\", 2185899))\n",
    "# testds.show()\n",
    "# print(testds.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ds.freq has raw frequency information, including AN, AC, and pop label. This is an array of \n",
    "structs, at indices determined by the categories in ds.globals.freq_index_dict\n",
    "\n",
    "ds.faf has filtered allele frequency information, including confidence intervals faf95 adn faf99.\n",
    "This is an array of structs, at indices determined by the category map in ds.globals.faf_index_dict\n",
    "\"\"\"\n",
    "\n",
    "def add_popmax_af(ds):\n",
    "    \"\"\"\n",
    "    Adds a popmax_faf and popmax_af_pop column to the ds Hail Table.\n",
    "    \n",
    "    popmax_faf is a faf structure from the original ds, containing the maximum faf of the\n",
    "    listed faf structures in the original ds, based on the filtering criteria \n",
    "    `default_faf_filter_type`. \n",
    "    \n",
    "    The popmax_index_dict_key column contains the text field from the\n",
    "    ds.globals.faf_index_dict which corresponds to each popmax_faf. This is similar to the\n",
    "    ds.popmax_faf.meta[\"pop\"] value but not exactly the same (gnomad_afr vs afr)\n",
    "    \n",
    "    Returns the updated ds.\n",
    "    \"\"\"\n",
    "    # Identify indices in FAF field that correspond to the entire dataset (not a subset like non-cancer)\n",
    "    # faf_index_map = [(k,v) for k, v in hl.eval(ds.globals.faf_index_dict).items() if k.startswith(\"gnomad_\")]\n",
    "    from enum import Enum\n",
    "    class FafFilterType(Enum):\n",
    "        # Each correponds to a filter func for a (k,v) of faf label to value\n",
    "        GNOMAD_GLOBAL = lambda t: t[0] == \"gnomad\"\n",
    "        GNOMAD_SUPERPOP = lambda t: t[0].startswith(\"gnomad_\")\n",
    "        ANY = lambda t: True\n",
    "\n",
    "    # By default, filter to superpopulations aggregate faf\n",
    "    default_faf_filter_type = FafFilterType.GNOMAD_SUPERPOP\n",
    "\n",
    "    def faf_filter(faf_idx_tuple:tuple):\n",
    "        return default_faf_filter_type(faf_idx_tuple)\n",
    "\n",
    "    # Get list of the global faf_index_dict which meets the default_faf_filter criteria\n",
    "    # This gives the indices of the desired populations, by default will take all top level populations\n",
    "    faf_index_map = list(filter(faf_filter, [(k,v) for k,v in hl.eval(ds.globals.faf_index_dict).items()]))\n",
    "    print(\"faf_index_map:\\n\" + str(faf_index_map))\n",
    "    faf_indices = [v for k,v in faf_index_map]\n",
    "    faf_labels = [k for k,v in faf_index_map]\n",
    "    \n",
    "    # freq_index_dict = gnomad.globals.freq_index_dict.collect()[0]\n",
    "    # freq_index_dict = dict(freq_index_dict.items())\n",
    "    # print(type(freq_index_dict))\n",
    "    # print(\"gnomad.globals.freq_index_dict:\\n\" + str(freq_index_dict))\n",
    "    \n",
    "    # Annotate table with popmax FAF\n",
    "    \n",
    "    # This only will return the maximum pop FAF for each\n",
    "    # variant, even if multiple populations meet the criteria. \n",
    "    # If we want all matching populations, need an explode() call\n",
    "    # to flatten the pop FAFs into a record per pop per variant\n",
    "    \n",
    "    faf_index_dicts = [hl.struct(label=k, index=v) for (k,v) in faf_index_map]\n",
    "    print(faf_index_dicts)\n",
    "    \n",
    "    # Add pop fafs sorted desc by faf95\n",
    "    ds = ds.annotate(\n",
    "        pop_fafs=hl.sorted(\n",
    "            # Take only the FAF entries that correspond to the desired populations (faf_indices)\n",
    "            # foreach faf_indices int, get the ds.faf entry for it\n",
    "            hl.literal(faf_index_dicts).map(\n",
    "                lambda i: hl.struct(faf=ds.faf[i.index],\n",
    "                                    label=i.label)\n",
    "            ),\n",
    "            # Sort by 95% confidence FAF\n",
    "            lambda faf_entry: faf_entry.faf.faf95,\n",
    "            # Sort high to low\n",
    "            reverse=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Replace pop fafs with only those with max value (allowing multiple to have same)\n",
    "    ds = ds.annotate(\n",
    "        popmax_fafs=hl.filter(\n",
    "            lambda faf_entry: faf_entry.faf.faf95 == ds.pop_fafs[0].faf.faf95,\n",
    "            ds.pop_fafs\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # If popmax empty or max (first in list) is zero, set to empty\n",
    "    # Remove popmax where AC=0\n",
    "    ds = ds.annotate(\n",
    "        popmax_fafs=hl.or_missing(\n",
    "            ds.popmax_fafs[0].faf.faf95 != 0.0,\n",
    "            hl.dict(ds.popmax_fafs.map(\n",
    "                lambda f: hl.tuple([f.label, f.faf])\n",
    "            ))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add number of pops which were popmax (edge case where multiple pop AC>0 have same faf)\n",
    "    ds = ds.annotate(\n",
    "        popmax_faf_count=ds.popmax_fafs.size() # DictExpression\n",
    "    )\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "        # Take the first entry with the highest FAF\n",
    "        #popmax_faf=ds.popmax_fafs[0],\n",
    "        popmax_faf_count=hl.len(ds.popmax_fafs),\n",
    "\n",
    "        # Label of the freq_index_dict entry for this record's max pop\n",
    "        # Take the first entry's label, which has the highest FAF\n",
    "        popmax_index_dict_keys=ds.popmax_fafs.keys() \n",
    "    )\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "        # ds.globals.freq_index_dict uses the same keys as ds.globals.faf_index_dict so\n",
    "        # we can reuse ds.popmax_index_dict_key created above\n",
    "        # ds.globals.freq_index_dict[ds.popmax_index_dict_key] is the index within the ds.globals.freq\n",
    "        # array that contains the frequency info for this population (subpop filtered)\n",
    "        #popmax_faf_pop_freqs=ds.freq[ds.globals.freq_index_dict[ds.popmax_index_dict_key]] \n",
    "        # Creates a map of pop label -> ds.freq struct \n",
    "        popmax_faf_pop_freqs=hl.dict(ds.popmax_index_dict_keys.map(\n",
    "            lambda key: hl.tuple([key, ds.freq[ds.globals.freq_index_dict[key]]]))))\n",
    "    \n",
    "    # Do some clean up work to pull each popmax record together.\n",
    "    # Makes it easier to get to each in case there are multiple popmaxes.\n",
    "    # ds.popmax will be an ArrayExpression of StructExpressions\n",
    "    ds = ds.annotate(\n",
    "        popmax=ds.popmax_index_dict_keys.map(\n",
    "            lambda popmax_key: hl.struct(\n",
    "                label=popmax_key,\n",
    "                faf=ds.popmax_fafs[popmax_key],\n",
    "                freq=ds.popmax_faf_pop_freqs[popmax_key])))\n",
    "    \n",
    "    # Drop fields just used for local computation, to reduce clutter\n",
    "    ds = ds.drop(\n",
    "        ds.pop_fafs,\n",
    "        ds.popmax_fafs,\n",
    "        ds.popmax_index_dict_keys,\n",
    "        ds.popmax_faf_pop_freqs)\n",
    "    return ds\n",
    "\n",
    "\n",
    "g = gnomad\n",
    "# g = g.filter(g.locus==hl.locus(\"1\", 155874156))\n",
    "gnomad_with_popmax = add_popmax_af(g)\n",
    "# more_than1_popmax = gnomad_with_popmax.filter(gnomad_with_popmax.popmax_faf_count > 1)\n",
    "# more_than1_popmax.show()\n",
    "\n",
    "# Done with these, drop them to reduce later clutter in output\n",
    "gnomad_with_popmax = gnomad_with_popmax.drop(\n",
    "    gnomad_with_popmax.freq,\n",
    "    gnomad_with_popmax.faf\n",
    ")\n",
    "gnomad_with_popmax.describe()\n",
    "gnomad_with_popmax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = gnomad_with_popmax.filter(gnomad_with_popmax.locus==hl.locus(\"1\", 10108))\n",
    "# print(t.freq[6].show()) # AC=1 AF=3.55e-3 AN=282\n",
    "# print(t.collect())\n",
    "\n",
    "# AC=218 AF=2.23e-2 AN=8710\n",
    "# t = gnomad_with_popmax.filter(gnomad_with_popmax.locus==hl.locus(\"1\", 155874156))\n",
    "# t.popmax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add gnomad-style coordinates\n",
    "gnomad_formatted = gnomad_with_popmax.annotate(\n",
    "#     source = gnomad_with_popmax.source,\n",
    "#     gene = ds.gene,\n",
    "#     popmax_pop = gnomad_with_popmax.popmax_faf.meta[\"pop\"],\n",
    "#     popmax_ac = gnomad_with_popmax.popmax_faf_pop_freq.AC,\n",
    "#     popmax_an = gnomad_with_popmax.popmax_faf_pop_freq.AN,\n",
    "#     popmax_faf95 = gnomad_with_popmax.popmax_faf.faf95,\n",
    "    genomic_coordinates = hl.format(\"%s-%s-%s-%s\",\n",
    "        gnomad_with_popmax.locus.contig,\n",
    "        hl.str(gnomad_with_popmax.locus.position),\n",
    "        gnomad_with_popmax.alleles[0],\n",
    "        gnomad_with_popmax.alleles[1]\n",
    "    )\n",
    ")\n",
    "# gnomad_formatted.filter(gnomad_formatted.popmax_ac != 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download clinvar BGZF\n",
    "import os, requests, subprocess\n",
    "\n",
    "# Function to download a file to a localpath.\n",
    "def download_to_file(url, filepath):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(filepath, \"wb\") as fout: \n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "             if chunk:\n",
    "                 fout.write(chunk)\n",
    "\n",
    "\n",
    "# This url always points to the latest dump file, updated periodically by ClinVar\n",
    "clinvar_vcf_url = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/clinvar.vcf.gz\"\n",
    "clinvar_vcf_localpath = \"/home/hail/clinvar.vcf.gz\"\n",
    "clinvar_vcf_hdfs = \"clinvar.vcf.gz\"\n",
    "# ClinVar VCF is small enough to download to dataproc default local disk.\n",
    "download_to_file(clinvar_vcf_url, clinvar_vcf_localpath)\n",
    "assert(os.path.exists(clinvar_vcf_localpath))\n",
    "print(\"Downloaded ClinVar VCF, file size (expecting ~28M): %d\" % os.path.getsize(clinvar_vcf_localpath))\n",
    "\n",
    "# Hail needs the file in HDFS\n",
    "local_to_hdfs(clinvar_vcf_localpath, clinvar_vcf_hdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ClinVar VCF into Hail Table\n",
    "clinvar = hl.import_vcf(\n",
    "    clinvar_vcf_hdfs,\n",
    "    force_bgz=True,\n",
    "    drop_samples=True, \n",
    "    skip_invalid_loci=True\n",
    ").rows()\n",
    "print(\"ClinVar row count: \" + str(clinvar.count()))\n",
    "clinvar.describe()\n",
    "clinvar.show()\n",
    "\n",
    "# ClinVar VCF export sets ID column to the ClinVar Variation ID (not rsid)\n",
    "# And sets the RS field of INFO to the rsid if it exists.\n",
    "# (https://ftp.ncbi.nlm.nih.gov/pub/clinvar/README_VCF.txt)\n",
    "# Hail then sets this ClinVar ID as the rsid column of the clinvar struct\n",
    "# We can filter to only the variants that exist in clinvar with:\n",
    "# gnomad_clinvar_ds = gnomad_clinvar_ds.filter(\n",
    "#     ~hl.is_missing(gnomad_clinvar_ds.clinvar_rsid)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load input file to join to\n",
    "filename = \"invitae-variant-input.tsv\"\n",
    "bucket_to_local(filename, filename)\n",
    "local_to_hdfs(filename, filename)\n",
    "variant_input = hl.import_table(filename, delimiter='\\t')\n",
    "variant_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join gnomad+clinvar to the input table\n",
    "# ClinVar HGVS is in clinvar.info.CLNHGVS string array\n",
    "c = clinvar.annotate(hgvs=clinvar.info.CLNHGVS)\n",
    "c = c.explode(c.hgvs)\n",
    "c = c.key_by(c.hgvs)\n",
    "c = c.repartition(500)\n",
    "c = c.persist()\n",
    "print(\"ClinVar partitions: \" + str(c.n_partitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join clinvar to variant input\n",
    "\n",
    "variant_input = variant_input.key_by(variant_input.Chromosomal_Variant)\n",
    "variant_input = variant_input.repartition(200)\n",
    "variant_input = variant_input.persist()\n",
    "print(variant_input.n_partitions())\n",
    "\n",
    "ds_joined = variant_input.annotate(\n",
    "    clinvar=c[variant_input.Chromosomal_Variant]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "ds_clinvar_missing = ds_joined.filter(hl.is_missing(ds_joined.clinvar))\n",
    "print(\"ClinVar missing from {} input variants\".format(ds_clinvar_missing.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds_joined2 = ds_joined.key_by(ds_joined.clinvar.locus, ds_joined.clinvar.alleles)\n",
    "# Evaluate the re-keying before join\n",
    "print(\"Persisting re-keyed ds_joined\")\n",
    "ds_joined2 = ds_joined2.persist()\n",
    "print(\"Annotating with gnomad\")\n",
    "ds_joined2 = ds_joined2.annotate(\n",
    "    gnomad=gnomad_formatted[ds_joined2.clinvar.locus, ds_joined2.clinvar.alleles]\n",
    ")\n",
    "# ds_joined2 = ds_joined2.persist()\n",
    "print(\"ds_joined2 n_partitions: \" + str(ds_joined2.n_partitions()))\n",
    "print(\"Describing\")\n",
    "ds_joined2.describe()\n",
    "# print(\"Showing\")\n",
    "# ds_joined2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_joined2.describe()\n",
    "# ds_joined2.show()\n",
    "ds_joined2.gnomad.popmax.freq.AC.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select desired output fields (columns are ordered as provided)\n",
    "output_ds = ds_joined2\n",
    "output_ds = output_ds.select(\n",
    "    output_ds.Input_Variant,\n",
    "    output_ds.Chromosomal_Variant,\n",
    "    gnomad_source=output_ds.gnomad.source,\n",
    "    gnomad_popmax_faf95=hl.delimit(output_ds.gnomad.popmax.faf.faf95),\n",
    "    gnomad_popmax_pop=hl.delimit(output_ds.gnomad.popmax.label),\n",
    "    gnomad_popmax_raw_ac=hl.delimit(output_ds.gnomad.popmax.freq.AC),\n",
    "    gnomad_popmax_raw_an=hl.delimit(output_ds.gnomad.popmax.freq.AN),\n",
    "    gnomad_genomic_coordinates=output_ds.gnomad.genomic_coordinates,\n",
    "    clinvar_variation_id=output_ds.clinvar.rsid,\n",
    "    clinvar_review_status=hl.delimit(output_ds.clinvar.info[\"CLNREVSTAT\"], \",\"),\n",
    "    clinvar_significance=hl.delimit(output_ds.clinvar.info[\"CLNSIG\"], \",\"),\n",
    "    clinvar_significance_interpretations=hl.delimit(output_ds.clinvar.info[\"CLNSIGCONF\"], \",\")\n",
    ")\n",
    "\n",
    "# For popmax with ac = 0, set to null\n",
    "# output_ds = output_ds.annotate(\n",
    "#     gnomad_popmax_pop=hl.or_missing(\n",
    "#         output_ds.gnomad_popmax_ac > 0,\n",
    "#         output_ds.gnomad_popmax_pop\n",
    "#     )\n",
    "# )\n",
    "\n",
    "output_ds.describe()\n",
    "# output_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug removal of 0 ac popmaxes\n",
    "# output_ds.filter(\n",
    "#     (~hl.is_missing(output_ds.gnomad_source)) \n",
    "#     & hl.is_missing(output_ds.gnomad_popmax_pop)\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TSV\n",
    "import time\n",
    "print(\"Starting export to %s\" % output_filename)\n",
    "start_time = time.time()\n",
    "output_ds.export(output_filename)\n",
    "end_time = time.time()\n",
    "print(\"Export took %.2f seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The export is in HDFS now, copy to machine-local file\n",
    "report_localpath = os.path.join(os.getcwd(), output_filename)\n",
    "os.system(\"rm %s\" % report_localpath)\n",
    "hdfs_to_local(output_filename, report_localpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Upload to bucket and filepath set at top of notebook\n",
    "gs_output_file = \"gs://%s/%s\" % (output_bucket, output_filename)\n",
    "local_to_bucket(report_localpath, gs_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
