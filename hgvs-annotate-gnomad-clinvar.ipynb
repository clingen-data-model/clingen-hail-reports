{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####\n",
    "# This notebook imports the following data sources:\n",
    "#  - gnomAD v2.1 hail table\n",
    "#  - ClinVar GRCh37 VCF\n",
    "# The update to gnomad v3 presented significant changes to the schema of metadata which \n",
    "# breaks the handling of AF and populations in this notebook.\n",
    "# It also renamed the contigs in the ref column which breaks the join with clinvar.\n",
    "#\n",
    "\"\"\"\n",
    "ClinVar:\n",
    "- clinvar_variation_id\n",
    "- clinvar_review_status\n",
    "- clinvar_significance\n",
    "- clinvar_significance_interpretations\n",
    "GnomAD:\n",
    "- faf95\n",
    "- source\n",
    "- popmax_pop\n",
    "- popmax_ac\n",
    "- popmax_an\n",
    "- genomic_coordinates\n",
    "- hgvs\n",
    "- protein_effect\n",
    "\"\"\"\n",
    "\n",
    "# Set the output bucket to write to, dataproc service account must have write access\n",
    "# Do not include trailing slash or \"gs://\"\n",
    "output_bucket = \"\"\n",
    "assert(len(output_bucket) > 0)\n",
    "# Set the TSV path to write into bucket. Can contain slash like \"folder/file.tsv\"\n",
    "# Do not include leading slash\n",
    "output_filename = \"report.tsv\"\n",
    "\n",
    "reference_genome = \"GRCh37\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "# `idempontent=True` is useful for running all cells in the notebook\n",
    "hl.init(idempotent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# utility functions for file placement\n",
    "import subprocess\n",
    "import os\n",
    "import time, datetime\n",
    "\n",
    "def run_args(args, fail_on_stderr=False, success_codes=[0]) -> tuple: # (stdout,stderr,returncode)\n",
    "    print(args)\n",
    "    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "    if (fail_on_stderr and len(stderr) > 0) or (p.returncode not in success_codes):\n",
    "        raise RuntimeError(\"command {} failed with code {}:{}\".format(\n",
    "            args, p.returncode, stderr))\n",
    "    return (stdout, stderr, p.returncode)\n",
    "\n",
    "def local_to_bucket(local_path:str, gcs_path:str):\n",
    "    if not gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = \"gs://{}/{}\".format(output_bucket, gcs_path)\n",
    "    args = [\"gsutil\", \"cp\", local_path, gcs_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def bucket_to_local(gcs_path:str, local_path:str):\n",
    "    if not gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = \"gs://{}/{}\".format(output_bucket, gcs_path)\n",
    "    args = [\"gsutil\", \"cp\", gcs_path, local_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def local_to_hdfs(local_path:str, hdfs_path:str):\n",
    "    if not local_path.startswith(\"/\"):\n",
    "        local_path = os.path.join(os.getcwd(), local_path)\n",
    "    args = [\"hdfs\", \"dfs\", \"-rm\", hdfs_path]\n",
    "    run_args(args, success_codes=[0,1]) # Allow error\n",
    "    args = [\"hadoop\", \"fs\", \"-cp\", \"file://\" + local_path, hdfs_path]\n",
    "#     args = [\"hdfs\", \"dfs\", \"-cp\", \"file://\" + local_path, hdfs_path]\n",
    "    run_args(args)\n",
    "    \n",
    "def hdfs_to_local(hdfs_path:str, local_path:str):\n",
    "    if os.path.exists(local_path):\n",
    "        os.remove(local_path)\n",
    "    args = [\"hdfs\", \"dfs\", \"-cp\", hdfs_path, \"file://\" + local_path]\n",
    "    run_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import io, re\n",
    "\n",
    "\n",
    "\n",
    "# thresh_reader = io.StringIO(thresholds)\n",
    "\n",
    "        \n",
    "# gene_thresholds, gene_refseqs = parse_thresholds(thresh_reader)\n",
    "\n",
    "# print(\"gene_thresholds:\\n%s\" % gene_thresholds)\n",
    "# print(\"input refseqs:\\n%s\" % gene_refseqs)\n",
    "\n",
    "# gene_thresholds_exp = hl.literal(gene_thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "# Read gnomAD data as Hail Tables\n",
    "# ds_exomes = hl.read_table(\"gs://gnomad-public/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\")\n",
    "# ds_exomes = hl.read_table(\n",
    "#     \"gs://gnomad-public-requester-pays/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht\")\n",
    "\n",
    "# ds_exomes = ds_exomes.annotate(\n",
    "#     source=\"gnomAD Exomes\"\n",
    "# )\n",
    "# ds_genomes = hl.read_table(\"gs://gnomad-public/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht\")\n",
    "ds_genomes = hl.read_table(\n",
    "    \"gs://gnomad-public-requester-pays/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht\")\n",
    "ds_genomes = ds_genomes.annotate(\n",
    "    source=\"gnomAD Genomes\"\n",
    ")\n",
    "\n",
    "# Can perform a union here if wanting both (ds = ds1.union(ds2))\n",
    "def select_necessary_cols(ds):\n",
    "    ds = ds.select(ds.freq, ds.faf, ds.vep, ds.source)\n",
    "    return ds\n",
    "\n",
    "# ds_exomes = select_necessary_cols(ds_exomes)\n",
    "ds_genomes = select_necessary_cols(ds_genomes)\n",
    "ds = ds_genomes\n",
    "# ds = ds_genomes.union(ds_exomes, unify=True)\n",
    "\n",
    "# Show the schema of the hail Table\n",
    "ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# START DEBUGGING CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "ds.describe()\n",
    "print(ds.globals.collect())\n",
    "testds = ds.filter(ds.locus==hl.locus(\"16\", 2185899))\n",
    "testds.show()\n",
    "print(testds.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# END DEBUGGING CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ds.freq has raw frequency information, including AN, AC, and pop label. This is an array of \n",
    "structs, at indices determined by the categories in ds.globals.freq_index_dict\n",
    "\n",
    "ds.faf has filtered allele frequency information, including confidence intervals faf95 adn faf99.\n",
    "This is an array of structs, at indices determined by the category map in ds.globals.faf_index_dict\n",
    "\"\"\"\n",
    "\n",
    "def add_popmax_af(ds):\n",
    "    \"\"\"\n",
    "    Adds a popmax_faf and popmax_af_pop column to the ds Hail Table.\n",
    "    \n",
    "    popmax_faf is a faf structure from the original ds, containing the maximum faf of the\n",
    "    listed faf structures in the original ds, based on the filtering criteria \n",
    "    `default_faf_filter_type`. \n",
    "    \n",
    "    The popmax_index_dict_key column contains the text field from the\n",
    "    ds.globals.faf_index_dict which corresponds to each popmax_faf. This is similar to the\n",
    "    ds.popmax_faf.meta[\"pop\"] value but not exactly the same (gnomad_afr vs afr)\n",
    "    \n",
    "    Returns the updated ds.\n",
    "    \"\"\"\n",
    "    # Identify indices in FAF field that correspond to the entire dataset (not a subset like non-cancer)\n",
    "    # faf_index_map = [(k,v) for k, v in hl.eval(ds.globals.faf_index_dict).items() if k.startswith(\"gnomad_\")]\n",
    "    from enum import Enum\n",
    "    class FafFilterType(Enum):\n",
    "        # Each correponds to a filter func for a (k,v) of faf label to value\n",
    "        GNOMAD_GLOBAL = lambda t: t[0] == \"gnomad\"\n",
    "        GNOMAD_SUPERPOP = lambda t: t[0].startswith(\"gnomad_\")\n",
    "        ANY = lambda t: True\n",
    "\n",
    "    # By default, filter to superpopulations aggregate faf\n",
    "    default_faf_filter_type = FafFilterType.GNOMAD_SUPERPOP\n",
    "\n",
    "    def faf_filter(faf_idx_tuple:tuple):\n",
    "        return default_faf_filter_type(faf_idx_tuple)\n",
    "\n",
    "    # Get list of the global faf_index_dict which meets the default_faf_filter criteria\n",
    "    # This gives the indices of the desired populations, by default will take all top level populations\n",
    "    faf_index_map = list(filter(faf_filter, [(k,v) for k,v in hl.eval(ds.globals.faf_index_dict).items()]))\n",
    "    print(\"faf_index_map:\\n\" + str(faf_index_map))\n",
    "    faf_indices = [v for k,v in faf_index_map]\n",
    "    faf_labels = [k for k,v in faf_index_map]\n",
    "    \n",
    "    # Annotate table with popmax FAF\n",
    "    \n",
    "    # This only will return the maximum pop FAF for each\n",
    "    # variant, even if multiple populations meet the criteria. \n",
    "    # If we want all matching populations, need an explode() call\n",
    "    # to flatten the pop FAFs into a record per pop per variant\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "        popmax_faf=hl.sorted(\n",
    "            # Take only the FAF entries that correspond to the desired populations (faf_indices)\n",
    "            hl.literal(faf_indices).map(lambda i: ds.faf[i]),\n",
    "            # Sort by 95% confidence FAF\n",
    "            lambda faf_entry: faf_entry.faf95,\n",
    "            # Sort high to low\n",
    "            reverse=True\n",
    "        )[0] # Take the first entry with the highest FAF\n",
    "        ,\n",
    "        # Label of the freq_index_dict entry for this record's max pop\n",
    "        popmax_index_dict_key=hl.sorted(\n",
    "            # List of tuples of (poplabel, faf_index)\n",
    "            list(zip(list(faf_labels), list(faf_indices))),\n",
    "\n",
    "            # Take only the FAF entries that correspond to the entire dataset\n",
    "            # Sort by 95% confidence FAF\n",
    "            key=lambda tpl: ds.faf[tpl[1]].faf95,\n",
    "            # Sort high to low\n",
    "            reverse=True\n",
    "        )[0][0] # Take the first entry's label, which has the highest FAF\n",
    "    )\n",
    "    \n",
    "    ds = ds.annotate(\n",
    "#         popmax_faf_pop_freq=ds.freq[ds.globals.freq_index_dict[\"gnomad_\" + ds.popmax_faf.meta.get(\"pop\")]]\n",
    "\n",
    "        # ds.globals.freq_index_dict uses the same keys as ds.globals.faf_index_dict so\n",
    "        # we can reuse ds.popmax_index_dict_key created above\n",
    "        popmax_faf_pop_freq=ds.freq[ds.globals.freq_index_dict[ds.popmax_index_dict_key]] \n",
    "    )\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "ds_with_popmax = add_popmax_af(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_with_popmax.describe()\n",
    "# ds_with_popmax.show()\n",
    "t = ds_with_popmax\n",
    "t = t.select(t.faf, t.popmax_faf, t.popmax_index_dict_key, t.popmax_faf_pop_freq)\n",
    "L = t.take(10)\n",
    "print(L[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ds_formatted = ds_with_popmax.select(\n",
    "    source = ds_with_popmax.source,\n",
    "#     gene = ds.gene,\n",
    "    popmax_pop = ds_with_popmax.popmax_faf.meta[\"pop\"],\n",
    "    popmax_ac = ds_with_popmax.popmax_faf_pop_freq.AC,\n",
    "    popmax_an = ds_with_popmax.popmax_faf_pop_freq.AN,\n",
    "    faf95 = ds_with_popmax.popmax_faf.faf95,\n",
    "    genomic_coordinates = hl.format(\"%s-%s-%s-%s\",\n",
    "        ds_with_popmax.locus.contig,\n",
    "        hl.str(ds_with_popmax.locus.position),\n",
    "        ds_with_popmax.alleles[0],\n",
    "        ds_with_popmax.alleles[1]\n",
    "    )\n",
    ")\n",
    "ds_formatted.filter(ds_formatted.popmax_ac != 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clinvar = hl.import_vcf(\"/path/to/clinvar.vcf.gz\", force_bgz=True, drop_samples=True, skip_invalid_loci=True).rows()\n",
    "\n",
    "# Download clinvar BGZF\n",
    "import os, requests, subprocess\n",
    "\n",
    "# Function to download a file to a localpath.\n",
    "def download_to_file(url, filepath):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(filepath, \"wb\") as fout: \n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "             if chunk:\n",
    "                 fout.write(chunk)\n",
    "\n",
    "\n",
    "# This url always points to the latest dump file, updated periodically by ClinVar\n",
    "clinvar_vcf_url = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/clinvar.vcf.gz\"\n",
    "clinvar_vcf_localpath = \"/home/hail/clinvar.vcf.gz\"\n",
    "clinvar_vcf_hdfs = \"clinvar.vcf.gz\"\n",
    "# ClinVar VCF is small enough to download to dataproc default local disk.\n",
    "download_to_file(clinvar_vcf_url, clinvar_vcf_localpath)\n",
    "assert(os.path.exists(clinvar_vcf_localpath))\n",
    "print(\"Downloaded ClinVar VCF, file size (expecting ~28M): %d\" % os.path.getsize(clinvar_vcf_localpath))\n",
    "\n",
    "# Hail needs the file in HDFS\n",
    "# def local_to_hdfs(local_path:str, hdfs_path:str):\n",
    "\n",
    "local_to_hdfs(clinvar_vcf_localpath, clinvar_vcf_hdfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ClinVar VCF into Hail Table\n",
    "clinvar = hl.import_vcf(\n",
    "    clinvar_vcf_hdfs,\n",
    "    force_bgz=True,\n",
    "    drop_samples=True, \n",
    "    skip_invalid_loci=True\n",
    ").rows()\n",
    "print(\"ClinVar row count: \" + str(clinvar.count()))\n",
    "\n",
    "# Join ClinVar table to gnomAD table. ClinVar fields available under the table.clinvar struct\n",
    "ds_gnomad_clinvar = ds_formatted.annotate(\n",
    "    clinvar=clinvar[ds_formatted.locus, ds_formatted.alleles]\n",
    ")\n",
    "\n",
    "ds_gnomad_clinvar.show()\n",
    "\n",
    "# ClinVar VCF export sets ID column to the ClinVar Variation ID (not rsid)\n",
    "# And sets the RS field of INFO to the rsid if it exists.\n",
    "# (https://ftp.ncbi.nlm.nih.gov/pub/clinvar/README_VCF.txt)\n",
    "# Hail then sets this ClinVar ID as the rsid column of the clinvar struct\n",
    "# We can filter to only the variants that exist in clinvar with:\n",
    "# gnomad_clinvar_ds = gnomad_clinvar_ds.filter(\n",
    "#     ~hl.is_missing(gnomad_clinvar_ds.clinvar_rsid)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ds_gnomad_clinvar to those in input set\n",
    "ds_gnomad_clinvar.filter(~hl.is_missing(ds_gnomad_clinvar.clinvar.info.CLNSIG)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load input file to join to\n",
    "filename = \"invitae-variant-input.tsv\"\n",
    "bucket_to_local(filename, filename)\n",
    "local_to_hdfs(filename, filename)\n",
    "variant_input = hl.import_table(filename, delimiter='\\t')\n",
    "# ds_filtered = ds_gnomad_clinvar.filter(\n",
    "#     hl.any() #TODO\n",
    "# )\n",
    "variant_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join gnomad+clinvar to the input table\n",
    "# ClinVar HGVS is in clinvar.info.CLNHGVS string array\n",
    "c = clinvar.annotate(hgvs=clinvar.info.CLNHGVS)\n",
    "c = c.explode(c.hgvs)\n",
    "c = c.key_by(c.hgvs)\n",
    "\n",
    "variant_input = variant_input.key_by(variant_input.Chromosomal_Variant)\n",
    "ds_joined = variant_input.annotate(\n",
    "    clinvar=c[variant_input.Chromosomal_Variant]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "ds_clinvar_missing = ds_joined.filter(hl.is_missing(ds_joined.clinvar))\n",
    "print(\"ClinVar missing from {} input variants\".format(ds_clinvar_missing.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_joined2 = ds_joined.key_by(ds_joined.clinvar.locus, ds_joined.clinvar.alleles)\n",
    "gnomad = ds_formatted\n",
    "ds_joined2 = ds_joined2.annotate(\n",
    "    gnomad=gnomad[ds_joined2.clinvar.locus, ds_joined2.clinvar.alleles]\n",
    ")\n",
    "ds_joined2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select desired output fields (columns are ordered as provided)\n",
    "output_ds = ds_joined2\n",
    "output_ds = output_ds.select(\n",
    "    output_ds.Input_Variant,\n",
    "    output_ds.Chromosomal_Variant,\n",
    "    gnomad_source=output_ds.gnomad.source,\n",
    "    gnomad_faf95=output_ds.gnomad.faf95,\n",
    "    gnomad_popmax_pop=output_ds.gnomad.popmax_pop,\n",
    "    gnomad_popmax_ac=output_ds.gnomad.popmax_ac,\n",
    "    gnomad_popmax_an=output_ds.gnomad.popmax_an,\n",
    "    gnomad_genomic_coordinates=output_ds.gnomad.genomic_coordinates,\n",
    "    clinvar_variation_id=output_ds.clinvar.rsid,\n",
    "    clinvar_review_status=hl.delimit(output_ds.clinvar.info[\"CLNREVSTAT\"], \",\"),\n",
    "    clinvar_significance=hl.delimit(output_ds.clinvar.info[\"CLNSIG\"], \",\"),\n",
    "    clinvar_significance_interpretations=hl.delimit(output_ds.clinvar.info[\"CLNSIGCONF\"], \",\")\n",
    ")\n",
    "\n",
    "output_ds.describe()\n",
    "\n",
    "# Export to TSV\n",
    "report_filename = \"report.tsv\"\n",
    "import time\n",
    "print(\"Starting export to %s\" % report_filename)\n",
    "start_time = time.time()\n",
    "output_ds.export(report_filename)\n",
    "end_time = time.time()\n",
    "print(\"Export took %.2f seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The export is in HDFS now, copy to machine-local file\n",
    "report_localpath = os.path.join(os.getcwd(), report_filename)\n",
    "os.system(\"rm %s\" % report_localpath)\n",
    "hdfs_to_local(report_filename, report_localpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Upload to bucket and filepath set at top of notebook\n",
    "gs_output_file = \"gs://%s/%s\" % (output_bucket, output_filename)\n",
    "local_to_bucket(report_localpath, gs_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
